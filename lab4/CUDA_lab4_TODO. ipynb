{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CUDA_lab4_TODO. ipynb","provenance":[{"file_id":"1xTEcXz3ZvxV2zgiZtlPTso8MQSPKt4Ze","timestamp":1647865560229},{"file_id":"15IDLiUMRJbKqZUZPccyigudINCD5uZ71","timestamp":1647557168268}],"collapsed_sections":["WoJbB3T5Vkw-","QadLRsY0hIga","D-xki0BeGIOY"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["---\n","# **LAB 4 - CUDA in Python**\n","---"],"metadata":{"id":"GdM-3tzilgXw"}},{"cell_type":"markdown","metadata":{"id":"WoJbB3T5Vkw-"},"source":["# ‚ñ∂Ô∏è CUDA setup"]},{"cell_type":"code","metadata":{"id":"Fht2Wy8wVkxJ"},"source":["!nvcc --version"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0jP2H_YJVkxJ"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["[GPU Compute Capability](https://developer.nvidia.com/cuda-gpus)"],"metadata":{"id":"VKbaxH9wWosO"}},{"cell_type":"code","metadata":{"id":"mhnDb4qS65Um"},"source":["from numba import cuda\n","\n","# getting device information\n","cuda.detect();"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vKtSX7K16op9"},"source":["from numba import cuda\n","\n","attribs = ['COMPUTE_CAPABILITY', 'MULTIPROCESSOR_COUNT', 'MAX_REGISTERS_PER_BLOCK', \n","         'MAX_SHARED_MEMORY_PER_BLOCK', ]\n","device = cuda.get_current_device()\n","#attribs = [s for s in dir(device) if s.isupper()]        # all attribs\n","for attr in attribs:\n","    print(attr, '=', getattr(device, attr))"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Bash setup\n","%%writefile /root/.bashrc\n","\n","# If not running interactively, don't do anything\n","[ -z \"$PS1\" ] && return\n","\n","# don't put duplicate lines in the history. See bash(1) for more options\n","# ... or force ignoredups and ignorespace\n","HISTCONTROL=ignoredups:ignorespace\n","\n","# append to the history file, don't overwrite it\n","shopt -s histappend\n","\n","# for setting history length see HISTSIZE and HISTFILESIZE in bash(1)\n","HISTSIZE=10000\n","HISTFILESIZE=20000\n","\n","# check the window size after each command and, if necessary,\n","# update the values of LINES and COLUMNS.\n","shopt -s checkwinsize\n","\n","# make less more friendly for non-text input files, see lesspipe(1)\n","[ -x /usr/bin/lesspipe ] && eval \"$(SHELL=/bin/sh lesspipe)\"\n","\n","PS1='\\[\\033[01;34m\\]\\w\\[\\033[00m\\]\\$ '\n","\n","# enable color support of ls and also add handy aliases\n","if [ -x /usr/bin/dircolors ]; then\n","    test -r ~/.dircolors && eval \"$(dircolors -b ~/.dircolors)\" || eval \"$(dircolors -b)\"\n","    alias ls='ls --color=auto'\n","    #alias dir='dir --color=auto'\n","    #alias vdir='vdir --color=auto'\n","\n","    alias grep='grep --color=auto'\n","    alias fgrep='fgrep --color=auto'\n","    alias egrep='egrep --color=auto'\n","fi\n","\n","# some more ls aliases\n","alias ll='ls -lF'\n","alias la='ls -A'\n","alias l='ls -CF'\n","\n","# path setup\n","export PATH=\"./:/usr/local/cuda/bin:$PATH\"\n","\n","!source /root/.bashrc"],"metadata":{"cellView":"form","id":"O8ICSyy8_GEq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Clone GPUcomputing site on github..."],"metadata":{"id":"DIEwSWSiTmCq"}},{"cell_type":"code","source":["!git clone https://github.com/giulianogrossi/GPUcomputing.git"],"metadata":{"id":"PNDBUaqqTd66"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üêç Add kernel"],"metadata":{"id":"QadLRsY0hIga"}},{"cell_type":"code","metadata":{"id":"aV8uMs78bIBD"},"source":["from numba import cuda\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# CUDA kernels written with `@cuda.jit` do not return values\n","@cuda.jit\n","def add_kernel(x, y, z):\n","  \"\"\"Perform vector sum z = x * y\n","  \"\"\"\n","    \n","  # The actual values of the following CUDA-provided variables for thread and block indices,\n","  # like function parameters, are not known until the kernel is launched.\n","  \n","  # This calculation gives a unique thread index within the entire grid (see the slides above for more)\n","  idx = cuda.grid(1)      # 1 = one dimensional thread grid, returns a single value.\n","                          # This Numba-provided convenience function is equivalent to\n","                          # `cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x`\n","\n","  # This thread will do the work on the data element with the same index as its own\n","  # unique index within the grid.\n","  z[idx] = x[idx] + y[idx]"],"metadata":{"id":"QrCQalOJm0AC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Define the kernel parameters..."],"metadata":{"id":"nPUwTxgiGOrt"}},{"cell_type":"code","source":["# Because of how we wrote the kernel above, we need to have a 1 thread to one data element mapping,\n","# therefore we define the number of threads in the grid (128*32) to equal n (4096).\n","threads_per_block = 128\n","blocks_per_grid = 32"],"metadata":{"id":"8Ic-tvzTGMsi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Generate data and device setup/transfert..."],"metadata":{"id":"G-Ou-SvFGiZg"}},{"cell_type":"code","source":["# populate data\n","n = 4096\n","x = np.arange(n).astype(np.int32) # [0...4095] on the host\n","y = np.ones_like(x)               # [1...1] on the host\n","\n","d_x = cuda.to_device(x) # Copy of x on the device\n","d_y = cuda.to_device(y) # Copy of y on the device\n","d_out = cuda.device_array_like(d_x) # Like np.array_like, but for device arrays\n"],"metadata":{"id":"dy4bV7iOGmmA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["launch the kernel..."],"metadata":{"id":"hYjgy8yNZ3N_"}},{"cell_type":"code","source":["%%time\n","add_kernel[blocks_per_grid, threads_per_block](d_x, d_y, d_out)\n","cuda.synchronize()\n","print(d_out.copy_to_host()) # Should be [1...4096]"],"metadata":{"id":"YTO_bWV0Zylu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D-xki0BeGIOY"},"source":["# üêç Mat multiplication kernel"]},{"cell_type":"code","metadata":{"id":"FBRTfOF5BIAK"},"source":["from numba import cuda\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vzoVtm6O61DL"},"source":["@cuda.jit\n","def matmul_gpu(A, B, C):\n","  \"\"\"Perform square matrix multiplication of C = A * B\n","  \"\"\"\n","\n","  row, col = cuda.grid(2)\n","    if row < C.shape[0] and col < C.shape[1]:\n","        tmp = 0.\n","        for k in range(A.shape[1]):\n","            tmp += A[row, k] * B[k, col]\n","        C[row, col] = tmp\n","\n","# matrix multiplication using the cpu and C-like programming style (** strongly discouraged **)\n","def matmul_cpu(A, B):\n","  \"\"\"Perform square matrix multiplication of C = A * B\n","  \"\"\"\n","  C = np.zeros((A.shape[1],B.shape[1]))\n","  for i in range(A.shape[1]):\n","    for j in range(B.shape[0]):\n","      tmp = 0.                            \n","      for k in range(A.shape[1]):\n","        tmp += A[i, k] * B[k, j]   # multiply elements in row i of A and column j of B and add to temp\n","      C[i, j] = tmp    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IGEoEUjo683n"},"source":["# generate random vals\n","np.random.seed(42)\n","SIZE = 1000\n","A = np.ones((SIZE,SIZE)).astype('float32')  # mat 1\n","B = np.ones((SIZE,SIZE)).astype('float32')  # mat 2\n","#C = np.zeros((SIZE,SIZE)).astype('float32')                       # mat where we store answer \n","\n","d_A = cuda.to_device(A) # Copy of A on the device\n","d_B = cuda.to_device(B) # Copy of B on the device\n","d_C = cuda.device_array_like(A) # malloc on the device\n","\n","# data type\n","d_A.dtype"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"id":"UK0F2rZlR4mK"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5bpfjqOWGrA5"},"source":["print(C)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Kernel parameters..."],"metadata":{"id":"zgJRscuQhrs4"}},{"cell_type":"code","metadata":{"id":"tlBglj4fGVCT"},"source":["block = (16, 16)  # each block will contain 16x16 threads, typically 128 - 512 threads/block\n","grid_x = int(np.ceil(C.shape[0] / block[0]))\n","grid_y = int(np.ceil(C.shape[1] / block[1]))\n","grid = (grid_x, grid_y)  # we calculate the gridsize (number of blocks) from array\n","print(grid)\n","print(f\"The kernel will be executed up to element {block[0]*grid_x}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ec9uXEHSGiex"},"source":["%%time\n","# execution of the kernel\n","matmul_gpu[grid, block](d_A, d_B, d_C)\n","cuda.synchronize()"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["C = d_C.copy_to_host()\n","print(C)"],"metadata":{"id":"7P7nxI9beoPD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","# execution of the python function in C-like programming style (strongly discouraged)\n","D = matmul_cpu(A, B)"],"metadata":{"id":"MNkQpD0XJ1mS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","# execution of the kernel\n","C = np.dot(A, B)"],"metadata":{"id":"gKK5r5tbL8Z9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(C)"],"metadata":{"id":"qigD6_LKgpOr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üêç Histogram"],"metadata":{"id":"m0phl-Kp-IpW"}},{"cell_type":"code","metadata":{"id":"39smdj2hlvE4"},"source":["from numba import cuda\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## üî¥ TODO"],"metadata":{"id":"4w-eKaICUut7"}},{"cell_type":"code","source":["@cuda.jit\n","def BMP_hist(...):\n","  # TODO"],"metadata":{"id":"Cmge3NGnNkfb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Gestione immagine e kernel..."],"metadata":{"id":"LEhX3FMsU4wZ"}},{"cell_type":"code","source":["from PIL import Image\n","\n","#image.save('beach1.bmp')\n","img = Image.open('GPUcomputing/images/dog.bmp')\n","img_rgb = img.convert('RGB')\n","img_rgb"],"metadata":{"id":"SFEoj0hudWKT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# converto to numpy (host)\n","I = np.asarray(img)\n","print(f\"Image size W x H x ch = {I.shape}\")\n","\n","# device data setup\n","d_I = cuda.to_device(I)\n","H = np.zeros((3,256)).astype(np.float32)\n","d_H = cuda.to_device(H)"],"metadata":{"id":"H6KKuOF0-J0Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Kernel parameters..."],"metadata":{"id":"YKvOMIuuiEY6"}},{"cell_type":"code","source":["block = (16, 16)  # each block will contain 16x16 threads, typically 128 - 512 threads/block\n","grid_x = int(np.ceil(I.shape[0] / block[0]))\n","grid_y = int(np.ceil(I.shape[1] / block[1]))\n","grid = (grid_x, grid_y)  # we calculate the gridsize (number of blocks) from array\n","print(grid)\n","print(f\"The kernel will be executed up to element {block[0]*grid_x}\")"],"metadata":{"id":"Of0_Z_U-h7r0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# kernel launch\n","\n","BMP_hist[grid, block](d_I, d_H)\n","hist = d_H.copy_to_host()\n","print(hist.shape)"],"metadata":{"id":"xSPvkDBhU7wD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","plt.bar(np.arange(256),hist[0])\n","plt.title('Histogram (R)')\n","plt.show()\n","plt.bar(np.arange(256),hist[1])\n","plt.title('Histogram (G)')\n","plt.show()\n","plt.bar(np.arange(256),hist[2])\n","plt.title('Histogram (B)')\n","plt.show()"],"metadata":{"id":"aPfeWSylWPRQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üî¥ TODO"],"metadata":{"id":"qksYnBHcuJGi"}},{"cell_type":"markdown","source":["Calcolare il prodotto di matrici MQDB con kernel CUDA in Python"],"metadata":{"id":"OO6JAs2hubiH"}}]}