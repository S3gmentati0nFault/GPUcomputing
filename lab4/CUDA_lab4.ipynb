{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"collapsed_sections":["NO_C5o9-xRF_","vXUIQkZLCTcG"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["---\n","# **LAB 4 - Shared memory (SMEM)**\n","---"],"metadata":{"id":"fZYqN0UwVLC_"}},{"cell_type":"markdown","metadata":{"id":"NO_C5o9-xRF_"},"source":["# ▶️ CUDA setup"]},{"cell_type":"code","metadata":{"id":"8fekR2O4xRGE"},"source":["!nvcc --version"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Psl9iouxRGE"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["GPU computing notebooks download (from github)"],"metadata":{"id":"gcg1GyK5srek"}},{"cell_type":"code","source":["!git clone https://github.com/giulianogrossi/GPUcomputing.git"],"metadata":{"id":"tyHOxci3s3H8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"227eLdP5csN1"},"source":["NVCC Plugin for Jupyter notebook"]},{"cell_type":"code","source":["%cd GPUcomputing/utils/nvcc4jupyter-master/\n","!!python3 -m build\n","%load_ext nvcc4jupyter\n","%cd /content/"],"metadata":{"id":"4TzxMBFds8aT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ✅ Parallel reduction con SMEM\n"],"metadata":{"id":"OHR7Zs3dNs1N"}},{"cell_type":"code","metadata":{"id":"B5QUNJdYauND"},"source":["%%cuda\n","#include <stdio.h>\n","#include <stdlib.h>\n","#include <assert.h>\n","#include \"/content/GPUcomputing/utils/common.h\"\n","\n","#define SMEM_DIM 1024\n","\n","\n","/*\n"," *  Block by block parallel implementation with divergence (sequential schema)\n"," */\n","__global__ void blockParReduce1(int *in, int *out, ulong n) {\n","\n","\tuint tid = threadIdx.x;\n","\tulong idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","\t// boundary check\n","\tif (idx >= n)\n","\t\treturn;\n","\n","\t// convert global data pointer to the local pointer of this block\n","\tint *thisBlock = in + blockIdx.x * blockDim.x;\n","\n","\t// in-place reduction in global memory\n","\tfor (int stride = 1; stride < blockDim.x; stride *= 2) {\n","\t\tif ((tid % (2 * stride)) == 0)\n","\t\t\tthisBlock[tid] += thisBlock[tid + stride];\n","\n","\t\t// synchronize within threadblock\n","\t\t__syncthreads();\n","\t}\n","\n","\t// write result for this block to global mem\n","\tif (tid == 0)\n","\t\tout[blockIdx.x] = thisBlock[0];\n","}\n","\n","/*\n"," *  Block by block parallel implementation without divergence (interleaved schema)\n"," */\n","__global__ void blockParReduce2(int *in, int *out, ulong n) {\n","\n","\tuint tid = threadIdx.x;\n","\tulong idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","\t// boundary check\n","\tif (idx >= n)\n","\t\treturn;\n","\n","\t// convert global data pointer to the local pointer of this block\n","\tint *thisBlock = in + blockIdx.x * blockDim.x;\n","\n","\t// in-place reduction in global memory\n","\tfor (int stride = blockDim.x / 2; stride > 0; stride >>= 1)  {\n","\t\tif (tid < stride)\n","\t\t\tthisBlock[tid] += thisBlock[tid + stride];\n","\n","\t\t// synchronize within threadblock\n","\t\t__syncthreads();\n","\t}\n","\n","\t// write result for this block to global mem\n","\tif (tid == 0)\n","\t\tout[blockIdx.x] = thisBlock[0];\n","}\n","\n","/*\n","    This version uses sequential addressing -- no divergence or bank conflicts.\n","*/\n","__global__ void blockParReduce_SMEM(int *in, int *out, ulong n) {\n","\n","\t// shared mem\n","\t__shared__ int smem[SMEM_DIM];\n","\n","\tunsigned int tid = threadIdx.x;\n","\tulong idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","\t// load shared mem\n","\tsmem[tid] = (idx < n) ? in[idx] : 0;\n","\n","\t// synchronize within threadblock\n","\t__syncthreads();\n","\n","\t// do reduction in shared mem\n","\tfor (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {\n","\t\tif (tid < stride)\n","\t\t\tsmem[tid] += smem[tid + stride];\n","\n","\t\t// synchronize within threadblock\n","\t\t__syncthreads();\n","\t}\n","\n","\t// write result for this block to global mem\n","\tif (tid == 0)\n","\t\tout[blockIdx.x] = smem[0];\n","}\n","\n","\n","/*\n"," * MAIN: test on parallel reduction\n"," */\n","int main(void) {\n","\tint *a, *b, *d_a, *d_b;\n","\tint blockSize = 1024;            // block dim 1D\n","\tulong numBlock = 1024*1024;      // grid dim 1D\n","\tulong n = blockSize * numBlock;  // array dim\n","\tfloat sum_CPU = 0.0, sum_GPU = 0.0;\n","\tlong nByte = n*sizeof(int), mByte = numBlock * sizeof(int);\n","\tdouble start, stopGPU, stopCPU, speedup;\n","\n","\tprintf(\"\\n****  test on parallel reduction  ****\\n\");\n","\n","\t// init\n","\ta = (int *) malloc(nByte);\n","\tb = (int *) malloc(mByte);\n","\tfor (ulong i = 0; i < n; i++) a[i] = 1;\n","\n","\tCHECK(cudaMalloc((void **) &d_a, nByte));\n","\tCHECK(cudaMalloc((void **) &d_b, mByte));\n","\tCHECK(cudaMemset((void *) d_b, 0, mByte));\n","\n","\t/***********************************************************/\n","\t/*                     CPU reduction                       */\n","\t/***********************************************************/\n","\tprintf(\"  Vector length: %.2f MB\\n\",n/(1024.0*1024.0));\n","\tprintf(\"\\n  CPU procedure...\\n\");\n","\tstart = seconds();\n","\tfor (ulong i = 0; i < n; i++)\n","    sum_CPU += a[i];\n","\tstopCPU = seconds() - start;\n","\tprintf(\"    Elapsed time: %f (sec) \\n\", stopCPU);\n","\tif (sum_CPU != n)\n","\t\tprintf(\"    ERROR: %f\\n\", sum_CPU);\n","\n","\tprintf(\"\\n  GPU kernels (mem required %lu bytes)\\n\", nByte);\n","\n","\t// reset input vector on GPU\n","\tCHECK(cudaMemcpy(d_a, a, nByte, cudaMemcpyHostToDevice));\n","\t/***********************************************************/\n","\t/*         KERNEL blockParReduce1 (divergent)              */\n","\t/***********************************************************/\n","\t// block by block parallel implementation with divergence\n","\tprintf(\"\\n  Launch kernel: blockParReduce1...\\n\");\n","\tstart = seconds();\n","\tblockParReduce1<<<numBlock, blockSize>>>(d_a, d_b, n);\n","\tCHECK(cudaGetLastError());\n","\tCHECK(cudaDeviceSynchronize());\n","\tstopGPU = seconds() - start;\n","\tspeedup = stopCPU/stopGPU;\n","\tprintf(\"    Elapsed time: %f (sec) - speedup %.1f\\n\", stopGPU,speedup);\n","\n","  // memcopy D2H\n","\tCHECK(cudaMemcpy(b, d_b, mByte, cudaMemcpyDeviceToHost));\n","\n","  // check result\n","\tsum_GPU = 0;\n","\tfor (uint i = 0; i < numBlock; i++)\n","\t\tsum_GPU += b[i];\n","\tif (sum_GPU != n)\n","\t\tprintf(\"    ERROR: %f\\n\", sum_GPU);\n","\n","\t// reset input vector on GPU\n","\tfor (ulong i = 0; i < n; i++) a[i]=1;\n","\tCHECK(cudaMemcpy(d_a, a, nByte, cudaMemcpyHostToDevice));\n","\n","\t/***********************************************************/\n","\t/*        KERNEL blockParReduce2  (non divergent)          */\n","\t/***********************************************************/\n","\t// block by block parallel implementation without divergence\n","\tprintf(\"\\n  Launch kernel: blockParReduce2...\\n\");\n","\tstart = seconds();\n","\tblockParReduce2<<<numBlock, blockSize>>>(d_a, d_b, n);\n","\tCHECK(cudaDeviceSynchronize());\n","\tstopGPU = seconds() - start;\n","\tspeedup = stopCPU/stopGPU;\n","\tprintf(\"    Elapsed time: %f (sec) - speedup %.1f\\n\", stopGPU,speedup);\n","\tCHECK(cudaGetLastError());\n","\n","  // memcopy D2H\n","\tCHECK(cudaMemcpy(b, d_b, mByte, cudaMemcpyDeviceToHost));\n","\n","  // check result\n","\tsum_GPU = 0;\n","\tfor (uint i = 0; i < numBlock; i++) {\n","\t\tsum_GPU += b[i];\n","\t}\n","\tif (sum_GPU != n)\n","\t\tprintf(\"    ERROR: %f\\n\", sum_GPU);\n","\n","  // reset input vector on GPU\n","\tfor (ulong i = 0; i < n; i++) a[i] = 1;\n","\tCHECK(cudaMemcpy(d_a, a, nByte, cudaMemcpyHostToDevice));\n","\n","\t/***********************************************************/\n","\t/*              KERNEL blockParReduce_SMEM                 */\n","\t/***********************************************************/\n","\t// block by block parallel implementation without divergence\n","\tprintf(\"\\n  Launch kernel: blockParReduce_SMEM...\\n\");\n","\tstart = seconds();\n","\tblockParReduce_SMEM<<<numBlock, blockSize>>>(d_a, d_b, n);\n","\tCHECK(cudaDeviceSynchronize());\n","\tstopGPU = seconds() - start;\n","\tspeedup = stopCPU/stopGPU;\n","\tprintf(\"    Elapsed time: %f (sec) - speedup %.1f\\n\", stopGPU, speedup);\n","\tCHECK(cudaGetLastError());\n","\n","  // memcopy D2H\n","\tCHECK(cudaMemcpy(b, d_b, mByte, cudaMemcpyDeviceToHost));\n","\n","  // check result\n","\tsum_GPU = 0;\n","\tfor (uint i = 0; i < numBlock; i++) {\n","\t\tsum_GPU += b[i];\n","\t}\n","\tif (sum_GPU != n)\n","\t\tprintf(\"    ERROR: %f\\n\", sum_GPU);\n","\n","\tcudaFree(d_a);\n","\tcudaFree(d_b);\n","\n","\treturn 0;\n","}\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vXUIQkZLCTcG"},"source":["# ✅ Moltiplicazione matriciale con SMEM\n"]},{"cell_type":"markdown","source":["**Prodotto di matrici con SMEM**\n","\n","Scrivere un programma CUDA per prodotto matrici $C = A*B$ che usi la SMEM e riduca così il 'traffico' in global mem\n","\n","**passi:**\n","1. Definire la SMEM per ogni blocco della matrice $C$\n","2. Svolgere un ciclo sui blocchi per caricare la SMEM da global mem\n","3. Sincronizzare -1-\n","4. Nel ciclo effettuare localmente all’interno di ogni blocco il calcolo del prodotto riga-colonna e caricare su registro\n","5. sincronizzare -2-\n","6. Scrivere il risultato finale su matrice prodotto in global mem\n"],"metadata":{"id":"Kdv736GrlSf1"}},{"cell_type":"markdown","source":["### ↘️ *`TODO...`*"],"metadata":{"id":"8B0w12MS4xtP"}},{"cell_type":"code","metadata":{"id":"-Y52R0d3CA50"},"source":["%%cuda\n","\n","#include <stdio.h>\n","#include <stdlib.h>\n","#include \"/content/GPUcomputing/utils/common.h\"\n","\n","#define IDX(i,j,n) (i*n+j)\n","#define ABS(x,y) (x-y>=0?x-y:y-x)\n","#define N 2048\n","#define P 2048\n","#define M 1024\n","#define BLOCK_SIZE 16\n","\n","\n","/*\n"," * Kernel for matrix product with static SMEM\n"," *      C  =  A  *  B\n"," *    (NxM) (MxP) (PxM)\n"," */\n","__global__ void matmulSMEMstatic(float* A, float* B, float* C) {\n","\t// indexes\n","\tuint row = blockIdx.y * blockDim.y + threadIdx.y;\n","\tuint col = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","\t// target: compute the right sum for the given row and col\n","\tfloat sum = 0.0;\n","\n","\t// static shared memory\n","\t__shared__ float As[BLOCK_SIZE][BLOCK_SIZE];\n","\t__shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];\n","\n","\t/*\tloop over blocks from block row of matrix A\n","\t  \tand block column of matrix B  */\n","\tuint numBlocks = (P + BLOCK_SIZE - 1) / BLOCK_SIZE;\n","\tfor (uint m = 0; m < numBlocks; m++) {\n","\n","\t\t// copy block from matrix to shared memory\n","\t\tuint r = m * BLOCK_SIZE + threadIdx.y;\n","\t\tuint c = m * BLOCK_SIZE + threadIdx.x;\n","\t\tAs[threadIdx.y][threadIdx.x] = A[IDX(row, c, P)];\n","\t\tBs[threadIdx.y][threadIdx.x] = B[IDX(r, col, M)];\n","\n","\t\t//---------------------------------------------------------------\n","\t\t__syncthreads();  //  BARRIER SYNC on SMEM loading\n","\t\t//---------------------------------------------------------------\n","\n","\t\t// length of this part of row-column product is BLOCK_SIZE\n","\t\t// except for last block when it may be smaller\n","\t\tuint K = BLOCK_SIZE;\n","\t\tif (m == numBlocks - 1) K = P - m * BLOCK_SIZE; // tune last block\n","\n","\t\t// compute this part of row-column product\n","\t\tfor (uint k = 0; k < K; k++)\n","\t\t\tsum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n","\n","\t\t//---------------------------------------------------------------\n","\t\t__syncthreads();  //  BARRIER SYNC on prod over blocks\n","\t\t//---------------------------------------------------------------\n","\t}\n","\n","\t// store computed element in matrix C\n","\tif (row < N && col < M)\n","\t\tC[IDX(row, col, M)] = sum;\n","}\n","\n","/*\n"," * Kernel for matrix product using dynamic SMEM\n"," */\n","__global__ void matmulSMEMdynamic(float* A, float* B, float* C, const uint SMEMsize) {\n","\t// indexes\n","\tuint row = blockIdx.y * blockDim.y + threadIdx.y;\n","\tuint col = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","\t// dynamic shared memory (inside or outside kernel)\n","\textern __shared__ float smem[];\n","\n","\t// Var As is manually set at beginning of shared\n","\tfloat *As = smem;\n","\t// Var Bs is manually set at the end of As\n","\tfloat *Bs = &smem[SMEMsize];\n","\n","\t// loop over blocks from block row of matrix A\n","\t// and block column of matrix B\n","\tfloat sum = 0.0;\n","\tuint numBlocks = (P + blockDim.x - 1) / blockDim.x;\n","\tfor (uint m = 0; m < numBlocks; m++) {\n","\n","\t\t// copy block from matrix to shared memory\n","\t\tuint c = m * blockDim.x + threadIdx.x;\n","\t\tuint r = m * blockDim.y + threadIdx.y;\n","\t\tAs[threadIdx.y * blockDim.y + threadIdx.x] = A[IDX(row, c, P)];\n","\t\tBs[threadIdx.y * blockDim.y + threadIdx.x] = B[IDX(r, col, M)];\n","\n","\t\t//---------------------------------------------------------------\n","\t\t__syncthreads();\n","\t\t//---------------------------------------------------------------\n","\n","\t\t// length of this part of row-column product is BLOCK_SIZE\n","\t\t// except for last block when it may be smaller\n","\t\tuint K = (m == numBlocks - 1 ? P - m * blockDim.x : blockDim.x);\n","\n","\t\t// compute this part of row-column product\n","\t\tfor (int k = 0; k < K; k++)\n","\t\t\tsum += As[threadIdx.y * blockDim.x + k] * Bs[k * blockDim.y + threadIdx.x];\n","\n","\t\t//---------------------------------------------------------------\n","\t\t__syncthreads();\n","\t\t//---------------------------------------------------------------\n","\t}\n","\n","\t// store computed element in matrix C\n","\tif (row < N && col < M)\n","\t\tC[IDX(row, col, M)] = sum;\n","}\n","\n","// functions definition\n","__global__ void matmul(float*, float*, float*);\n","void matmulCPU(float*, float*, float*);\n","void checkResult(float*, float*);\n","\n","\n","/*\n"," * MAIN\n"," */\n","int main(void) {\n","\t // Kernels for matrix product\n","\t //      C  =  A  *  B\n","\t //    (NxM) (NxP) (PxM)\n","\tprintf(\"N = %d, M = %d, K = %d\\n\", N, M, P);\n","\tuint rowA = N, rowB = P;\n","\tuint colA = P, colB = M;\n","\tuint rowC = N, colC = M;\n","\tfloat *A, *B, *C, *C1;\n","\tfloat *dev_A, *dev_B, *dev_C;\n","\n","\t// dims\n","\tunsigned long Asize = rowA * colA * sizeof(float);\n","\tunsigned long Bsize = rowB * colB * sizeof(float);\n","\tunsigned long Csize = rowC * colC * sizeof(float);\n","\n","\t// malloc host memory\n","\tA = (float*) malloc(Asize);\n","\tB = (float*) malloc(Bsize);\n","\tC = (float*) malloc(Csize);\n","\tC1 = (float*) malloc(Csize);\n","\n","\t// malloc device memory\n","\tCHECK(cudaMalloc((void** )&dev_A, Asize));\n","\tCHECK(cudaMalloc((void** )&dev_B, Bsize));\n","\tCHECK(cudaMalloc((void** )&dev_C, Csize));\n","\tprintf(\"Total amount of allocated memory on GPU %.2f MB\\n\\n\", (float)(Asize + Bsize + Csize)/(1024.0*1024.0));\n","\n","\t// fill the matrices A and B\n","\tfor (int i = 0; i < N * P; i++) A[i] = 1.0;\n","\tfor (int i = 0; i < P * M; i++) B[i] = 1.0;\n","\n","\t/***********************************************************/\n","\t/*                       CPU matmul                       */\n","\t/***********************************************************/\n","\tprintf(\"\\n   *** CPU & NAIVE KERNEL ***\\n\\n\");\n","\tdouble start = seconds();\n","\tmatmulCPU(A, B, C);\n","\tdouble cpu_time = seconds() - start;\n","\tprintf(\"   matmul elapsed time CPU = %f\\n\\n\", cpu_time);\n","\n","\n","\t// copy matrices A and B to the GPU\n","\tCHECK(cudaMemcpy(dev_A, A, Asize, cudaMemcpyHostToDevice));\n","\tCHECK(cudaMemcpy(dev_B, B, Bsize, cudaMemcpyHostToDevice));\n","\n","\t/***********************************************************/\n","\t/*                    GPU naive matmul                     */\n","\t/***********************************************************/\n","\t// grid block dims = shared mem dims = BLOCK_SIZE\n","\tdim3 block(BLOCK_SIZE, BLOCK_SIZE);\n","\tdim3 grid((M + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n","\tstart = seconds();\n","\tmatmul<<<grid, block>>>(dev_A, dev_B, dev_C);\n","\tCHECK(cudaDeviceSynchronize());\n","\tdouble gpu_time1 = seconds() - start;\n","\tprintf(\"   Kernel naive matmul elapsed time GPU = %f\\n\", gpu_time1);\n","\tprintf(\"   - Speed-up                           = %f\\n\", cpu_time / gpu_time1);\n","\n","\t// copy the array 'C' back from the GPU to the CPU\n","\tCHECK(cudaMemcpy(C1, dev_C, Csize, cudaMemcpyDeviceToHost));\n","\tcheckResult(C,C1);\n","\tCHECK(cudaMemset((void *) dev_C, 0, Csize));\n","\n","\t/***********************************************************/\n","\t/*              GPU matmulSMEM static SMEM                 */\n","\t/***********************************************************/\n","\t// grid block dims = shared mem dims = BLOCK_SIZE\n","\tprintf(\"\\n   *** USING STATIC SMEM ***\\n\\n\");\n","\tstart = seconds();\n","\tmatmulSMEMstatic<<<grid, block>>>(dev_A, dev_B, dev_C);\n","\tCHECK(cudaDeviceSynchronize());\n","\t double gpu_time2 = seconds() - start;\n","\tprintf(\"   Kernel matmulSMEM static elapsed time GPU = %f\\n\", gpu_time2);\n","\tprintf(\"   - Speed-up                                = %f\\n\", gpu_time1 / gpu_time2);\n","\n","\t// copy the array 'C' back from the GPU to the CPU\n","\tCHECK(cudaMemcpy(C1, dev_C, Csize, cudaMemcpyDeviceToHost));\n","\tcheckResult(C,C1);\n","\tCHECK(cudaMemset((void *) dev_C, 0, Csize));\n","\n","\t/***********************************************************/\n","\t/*            GPU matmulSMEMD dynamic SMEM                */\n","\t/***********************************************************/\n","\t// set cache size\n","\tcudaDeviceSetCacheConfig (cudaFuncCachePreferShared);\n","\tprintf(\"\\n   *** USING DYNAMIC SMEM ***\\n\\n\");\n","\n","\t// try with various SMEM sizes\n","\tuint sizes[] = {8, 16, 32};\n","\tfor (int i = 0; i < 3; i++) {\n","\t\tuint blockSize = sizes[i];\n","\t\tblock.x = blockSize;\n","\t\tblock.y = blockSize;\n","\t\tgrid.x = (M + block.x - 1) / block.x;\n","\t\tgrid.y = (N + block.y - 1) / block.y;\n","\t\tuint SMEMsize = blockSize * blockSize;\n","\t\tuint SMEMbyte = 2 * SMEMsize * sizeof(float);\n","\t\tstart = seconds();\n","\t\tmatmulSMEMdynamic<<< grid, block, SMEMbyte >>>(dev_A, dev_B, dev_C, SMEMsize);\n","\t\tCHECK(cudaDeviceSynchronize());\n","\t\tprintf(\"   Kernel matmulSMEM dynamic (SMEM size %d) elapsed time GPU = %f\\n\", blockSize, seconds() - start);\n","\n","\t\t// amount of SMEM used\n","\t\t//printf(\"   Total amount of shared memory required per block %.1f KB\\n\", (float) SMEMbyte / (float) 1024);\n","\n","\t\t// copy the array 'C' back from the GPU to the CPU\n","\t\tCHECK(cudaMemcpy(C1, dev_C, Csize, cudaMemcpyDeviceToHost));\n","\t\tcheckResult(C,C1);\n","\t\tCHECK(cudaMemset((void *) dev_C, 0, Csize));\n","\t}\n","\n","\t// free the memory allocated on the GPU\n","\tcudaFree(dev_A);\n","\tcudaFree(dev_B);\n","\tcudaFree(dev_C);\n","\n","\tcudaDeviceReset();\n","\treturn EXIT_SUCCESS;\n","}\n","\n","// Kernel for naive matrix product\n","__global__ void matmul(float* A, float* B, float* C) {\n","\t// indexes\n","\tint row = blockIdx.y * blockDim.y + threadIdx.y;\n","\tint col = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","\t// each thread computes an entry of the product matrix\n","\tif ((row < N) && (col < M)) {\n","\t\tfloat sum = 0;\n","\t\tfor (int k = 0; k < P; k++)\n","\t\t\tsum += A[IDX(row, k, P)] * B[IDX(k, col, M)];\n","\t\tC[IDX(row, col, M)] = sum;\n","\t}\n","}\n","\n","// matrix product on CPU\n","void matmulCPU(float* A, float* B, float* C) {\n","\tfor (int row = 0; row < N; row++)\n","\t\tfor (int col = 0; col < M; col++) {\n","\t\t\tfloat sum = 0;\n","\t\t\tfor (int k = 0; k < P; k++)\n","\t\t\t\tsum += A[IDX(row, k, P)] * B[IDX(k, col, M)];\n","\t\t\tC[IDX(row, col, M)] = sum;\n","\t\t}\n","}\n","\n","// Elementwise comparison between two mqdb\n","void checkResult(float *A, float *B) {\n","\tdouble epsilon = 1.0E-8;\n","\tbool match = 1;\n","\tfor (int i = 0; i < N*M; i++)\n","\t\tif (ABS(A[i], B[i]) > epsilon) {\n","\t\t\tmatch = 0;\n","\t\t\tprintf(\"   * Arrays do not match!\\n\");\n","\t\t\tbreak;\n","\t\t}\n","\tif (!match)\n","\t\tprintf(\"   Arrays do not match\\n\\n\");\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0PSc9B9PDTWt"},"source":["# Compilazione ed esecuzione\n","\n","!nvcc -arch=sm_75 src/matmulSMEM.cu  -o matmulSMEM\n","!matmulSMEM"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mfR471rze2p-"},"source":["!ls -la"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SOFMQZAkjlLW"},"source":["# ✅ Convoluzione con SMEM"]},{"cell_type":"code","metadata":{"id":"v9nRkLgeB10A"},"source":["%%cuda_group_save --name \"conv1D.cu\" --group \"SMEM\"\n","#include <stdlib.h>\n","#include <stdio.h>\n","#include \"/content/GPUcomputing/utils/common.h\"\n","\n","#define MASK_RADIUS  500\n","#define MASK_SIZE    2 * MASK_RADIUS + 1\n","#define BLOCK_SIZE   1024\n","#define TILE_SIZE    BLOCK_SIZE + MASK_SIZE - 1\n","\n","\n","__device__ __constant__ float d_mask[MASK_SIZE];\n","\n","// functions definition\n","void initialData(float*, int);\n","void movingAverage(float*, int n);\n","void printData(float*, const int);\n","void convolutionHost(float*, float*, float*, const int);\n","void checkResult(float*, float*, int);\n","\n","/*\n"," * kernel for 1D convolution: it holds only if MASK_RADIUS < BLOCK_SIZE\n"," */\n","__global__ void conv1D(float *result, float *data, int n) {\n","\tunsigned int i = blockDim.x * blockIdx.x + threadIdx.x;\n","\n","\t// shared memory size = BLOCK_SIZE + MASK\n","\t__shared__ float tile[TILE_SIZE];\n","\n","\t// boundary\n","\tint left = blockIdx.x * blockDim.x - MASK_RADIUS;\n","\tint right = (blockIdx.x + 1) * blockDim.x;\n","\n","  // left halo\n","\tif (threadIdx.x < MASK_RADIUS)\n","\t\ttile[threadIdx.x] = left < 0 ? 0 : data[left + threadIdx.x];\n","\n","  // center\n","\ttile[threadIdx.x + MASK_RADIUS] = data[i];\n","\n","  // right halo\n","\tif (threadIdx.x >= blockDim.x - MASK_RADIUS)\n","\t\ttile[threadIdx.x + MASK_SIZE - 1] = right >= n ? 0 : data[right + threadIdx.x - blockDim.x + MASK_RADIUS];\n","\n","\t__syncthreads();\n","\n","\t// convolution: tile * mask\n","\tfloat sum = 0;\n","\tfor (int i = -MASK_RADIUS; i <= MASK_RADIUS; i++)\n","\t\tsum += tile[threadIdx.x + MASK_RADIUS + i] * d_mask[i + MASK_RADIUS];\n","\n","\t// final result\n","\tresult[i] = sum;\n","}\n","\n","/*\n"," * Basic kernel for 1D convolution\n"," */\n","__global__ void conv1D_basic(float *result, float *data, int n) {\n","\n","\tunsigned int i = blockDim.x * blockIdx.x + threadIdx.x;\n","\tfloat sum = 0;\n","\n","\t// convolution of tile size elements\n","  int start_point = i - MASK_RADIUS;\n","\tfor (int j = 0; j < MASK_SIZE; j++) {\n","    if (start_point + j >= 0 && start_point + j < n)\n","      sum += data[start_point + j] * d_mask[j];\n","  }\n","\n","\t// final result\n","\tresult[i] = sum;\n","}\n","\n","\n","/*\n"," * MAIN: convolution 1D host & device\n"," */\n","int main(int argc, char **argv) {\n","\n","\t// set up array size\n","\tint n = 1 << 25;\n","\tint N = MASK_SIZE;\n","\n","\tprintf(\"Array of size = %.1f MB\\n\", n/(1024.0*1024.0));\n","\tprintf(\"Mask size     = %d elements\\n\\n\", N);\n","\n","\t// mem sizes\n","\tsize_t nBytes = n * sizeof(float);\n","\tsize_t nBytes_mask = N * sizeof(float);\n","\n","\t// grid configuration\n","\tdim3 block(BLOCK_SIZE);\n","\tdim3 grid((n + BLOCK_SIZE - 1) / BLOCK_SIZE);\n","\n","\t// allocate host memory\n","\tfloat *h_data = (float *) malloc(nBytes);\n","\tfloat *h_result = (float *) malloc(nBytes);\n","\tfloat *h_result_basic = (float *) malloc(nBytes);\n","\tfloat *result = (float *) malloc(nBytes);\n","\tfloat *h_mask = (float *) malloc(nBytes_mask);\n","\n","\t//  initialize host array\n","\tmovingAverage(h_mask, N);\n","\tinitialData(h_data, n);\n","\n","  /***********************************************************/\n","\t/*               convolution on host                       */\n","\t/***********************************************************/\n","\tdouble start = seconds();\n","\tconvolutionHost(h_data, result, h_mask, n);\n","\tdouble hostElaps = seconds() - start;\n","\n","\t/***********************************************************/\n","\t/*               convolution on device                     */\n","\t/***********************************************************/\n","\t// allocate device memory\n","\tfloat *d_data, *d_result;\n","\tCHECK(cudaMalloc((void**)&d_data, nBytes));\n","\tCHECK(cudaMalloc((void**)&d_result, nBytes));\n","\n","\t// copy data from host to device\n","\tCHECK(cudaMemcpy(d_data, h_data, nBytes, cudaMemcpyHostToDevice));\n","\tCHECK(cudaMemcpyToSymbol(d_mask, h_mask, nBytes_mask));\n","\n","\tstart = seconds();\n","\tconv1D<<<grid, block>>>(d_result, d_data, n);\n","\tCHECK(cudaDeviceSynchronize());\n","\tdouble devElaps = seconds() - start;\n","\n","\t// check result\n","\tCHECK(cudaMemcpy(h_result, d_result, nBytes, cudaMemcpyDeviceToHost));\n","\tcheckResult(h_result, result, n);\n","\n","\t/***********************************************************/\n","\t/*            convolution on device basic                  */\n","\t/***********************************************************/\n","\tstart = seconds();\n","\tconv1D_basic<<<grid, block>>>(d_result, d_data, n);\n","\tCHECK(cudaDeviceSynchronize());\n","\tdouble devElaps1 = seconds() - start;\n","\n","\t// check result\n","\tCHECK(cudaMemcpy(h_result_basic, d_result, nBytes, cudaMemcpyDeviceToHost));\n","\tcheckResult(h_result_basic, result, n);\n","\n","\t// print exec times\n","\tprintf(\"Times:\\n\");\n","\tprintf(\"   - CPU elapsed time         = %f\\n\", hostElaps);\n","  printf(\"   - GPU elapsed time (SMEM)  = %f\\n\", devElaps);\n","\tprintf(\"   - GPU elapsed time (basic) = %f\\n\", devElaps1);\n","  printf(\"   - Speed-up (H/SMEM)        = %f\\n\", hostElaps / devElaps);\n","\tprintf(\"   - Speed-up (basic/SMEM)    = %f\\n\", devElaps1 / devElaps);\n","\n","\n","\t// free host and device memory\n","\tCHECK(cudaFree(d_result));\n","\tCHECK(cudaFree(d_data));\n","\n","\treturn EXIT_SUCCESS;\n","}\n","\n","void initialData(float *h_data, int n) {\n","\t// initialize the data\n","\tfor (int i = 0; i < n; i++)\n","\t\th_data[i] = 1.0;\n","}\n","\n","void movingAverage(float *h_mask, int n) {\n","\t// initialize mask moving average\n","\tfor (int i = 0; i < n; i++)\n","\t\th_mask[i] = 1.0 / ((float) n);\n","\treturn;\n","}\n","\n","void printData(float *a, const int size) {\n","\tprintf(\"\\n\");\n","\tfor (int i = 0; i < size; i++)\n","\t\tprintf(\"%.2f \", a[i]);\n","\tprintf(\"\\n\");\n","\treturn;\n","}\n","\n","void convolutionHost(float *data, float *result, float *mask, const int n) {\n","\tfor (int i = 0; i < n; i++) {\n","\t\tfloat sum = 0;\n","\t\tfor (int j = 0; j < MASK_SIZE; j++) {\n","\t\t\tint idx = i - MASK_RADIUS + j;\n","\t\t\tif (idx >= 0 && idx < n)\n","\t\t\t\tsum += data[idx] * mask[j];\n","\t\t}\n","\t\tresult[i] = sum;\n","\t}\n","}\n","\n","void checkResult(float *d_result, float *h_result, int n) {\n","\tdouble epsilon = 1.0E-8;\n","\n","\tfor (int i = 0; i < n; i++)\n","\t\tif (abs(h_result[i] - d_result[i]) > epsilon) {\n","\t\t\tprintf(\"different on entry (%d) |h_result - d_result| >  %f\\n\", i, epsilon);\n","\t\t\tbreak;\n","\t\t}\n","}\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wLxZjCx8bT3s"},"source":["# Compilazione ed esecuzione\n","!nvcc -arch=sm_75  src/SMEM/conv1D.cu -o conv1D\n","!./conv1D"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Convoluzione 2D..."],"metadata":{"id":"xYysz72z216s"}},{"cell_type":"markdown","source":["### ↘️ *`TODO...`*"],"metadata":{"id":"hI7kdGlx72HO"}},{"cell_type":"code","source":["%%cuda\n","#include <stdlib.h>\n","#include <string.h>\n","#include \"/content/GPUcomputing/utils/common.h\"\n","\n","#define DATA_WIDTH   (5*1024)\n","#define DATA_HEIGHT  (5*1024)\n","#define BLOCK_SIZE   32\n","#define MASK_RADIUS  10\n","#define MASK_WIDTH   (2 * MASK_RADIUS + 1)\n","#define TILE_WIDTH   (BLOCK_SIZE + MASK_WIDTH - 1)\n","#define DEBUG 0\n","\n","// constant mem\n","__constant__ float M_dev[MASK_WIDTH * MASK_WIDTH];\n","\n","/*\n"," * Basic kernel for 2D convolution\n"," */\n","__global__ void conv2D_basic(float* A, float* B) {\n","\n","\t//index computation\n","\tint x = blockIdx.x * blockDim.x + threadIdx.x;\n","\tint y = blockIdx.y * blockDim.y + threadIdx.y;\n","\n","\tif(x < DATA_WIDTH && y < DATA_WIDTH){\n","\n","\t\tfloat result = 0.0f;\n","\n","\t\t//start index on the input array\n","\t\tint startX = x - MASK_RADIUS;\n","\t\tint startY = y - MASK_RADIUS;\n","\n","\t\t//compute convolution\n","\t\tfor (int i = 0; i < MASK_WIDTH; ++i) {\n","\t\t\tfor(int j = 0; j < MASK_WIDTH; j++ ) {\n","\n","\t\t\t\t//boundary check\n","\t\t\t\tif((startY + i >= 0) && (startY + i < DATA_WIDTH) && (startX + j >= 0) && (startX + j < DATA_WIDTH))\n","\t\t\t\t\tresult += A[((startY + i) * DATA_WIDTH) + (startX + j)] * M_dev[(i * MASK_WIDTH) + j];\n","\t\t\t}\n","\t\t}\n","\t\t//store final value\n","\t\tB[(y * DATA_WIDTH) + x] = result;\n","\t}\n","}\n","\n","/*\n"," * kernel for convolution 2D (it holds only if MASK_RADIUS < BLOCK_SIZE)\n"," */\n","__global__ void conv2D_SMEM(float *A, float *B) {\n","\tint x = blockIdx.x * blockDim.x + threadIdx.x;\n","\tint y = blockIdx.y * blockDim.y + threadIdx.y;\n","\tint RAD = MASK_RADIUS;\n","  int BmR = BLOCK_SIZE - RAD;\n","  int W = DATA_WIDTH;\n","  int H = DATA_HEIGHT;\n","\tint m = MASK_WIDTH;\n","\n","\t// shared mem\n","\t__shared__ float A_s[TILE_WIDTH][TILE_WIDTH];\n","\n","  // START SHARED MEMORY LOADING\n","\n","  // 1. copy the tile upper halo\n","  if ((threadIdx.y < RAD) ) {\n","\n","    // left corner\n","    if (threadIdx.x < RAD && (x-RAD) >= 0 && (y-RAD) >= 0)\n","      A_s[threadIdx.y][threadIdx.x] = A[(y-RAD) * W + x - RAD];\n","\n","    // right corner\n","    if (threadIdx.x >= BmR && (x+RAD) < W && (y-RAD) >= 0)\n","      A_s[threadIdx.y][threadIdx.x + 2*RAD] = A[(y-RAD) * W + x + RAD];\n","\n","    // edge\n","    if ((y-RAD) >= 0)\n","      A_s[threadIdx.y][threadIdx.x + RAD] = A[(y-RAD) * W + x ];\n","  }\n","\n","  // 2. copy the tile bottom halo\n","  if (threadIdx.y >= BmR) {\n","\n","    // left corner\n","    if (threadIdx.x < RAD && (x-RAD) >= 0 && (y+RAD) < H)\n","      A_s[threadIdx.y + 2*RAD][threadIdx.x] = A[(y+RAD) * W + x - RAD];\n","\n","    // right corner\n","    if (threadIdx.x >= BmR && (y+RAD) < H)\n","      A_s[threadIdx.y + 2*RAD][threadIdx.x + 2*RAD] = A[(y+RAD) * W + x + RAD];\n","\n","    // edge\n","    if ((y+RAD) < H)\n","      A_s[threadIdx.y + 2*RAD][threadIdx.x + RAD] = A[(y+RAD) * W + x];\n","  }\n","\n","  // 3. copy the tile left-edge halo\n","  if (threadIdx.x < RAD)\n","    // edge\n","    if ((x-RAD) >= 0)\n","      A_s[threadIdx.y + RAD][threadIdx.x] = A[y * W + x - RAD];\n","\n","  // 4. copy the tile right-edge halo\n","  if (threadIdx.x >= BmR)\n","    // edge\n","    if ((x+RAD) < W)\n","      A_s[threadIdx.y + RAD][threadIdx.x + 2*RAD] = A[y * W + x + RAD];\n","\n","\n","  // 5. copy the tile center <-> block\n","\tA_s[RAD + threadIdx.y][RAD + threadIdx.x] = A[y*W+x];\n","\n","  // END SHARED MEMORY LOADING\n","\n","\t__syncthreads();\n","\n","\tfloat conv_sum = 0.0;\n","\tfor (int i = 0; i < m; i++)\n","\t\tfor (int j = 0; j < m; j++)\n","\t\t\tconv_sum += A_s[threadIdx.y+i][threadIdx.x+j] * M_dev[i*m + j];\n","\n","  // store conv result\n","  B[x*W+y] = conv_sum;\n","}\n","\n","// functions definition\n","void conv2D_host(float*, float*, const float*);\n","void Avg_mask(float*);\n","\n","\n","/*\n"," * main\n"," */\n","int main(void) {\n","\n","\n","\tint nW = DATA_WIDTH;\n","  int nH = DATA_HEIGHT;\n","\tint b = BLOCK_SIZE;\n","\n","\tfloat M[MASK_WIDTH * MASK_WIDTH]; // const size\n","\tfloat *A, *B, *A_dev, *B_dev;\n","\tint datasize = nW * nH * sizeof(float);\n","  int masksize = MASK_WIDTH * MASK_WIDTH * sizeof(float);\n","\n","  printf(\"Data size: %.2f (MB)\\n\", (float)datasize/(1024.0*1024.0));\n","\tprintf(\"Initializing data...\\n\");\n","\tA = (float *) malloc(datasize);\n","\tB = (float *) malloc(datasize);\n","\n","\t// initialize data\n","\tfor (int i = 0; i < nH; i++)\n","\t\tfor (int j = 0; j < nW; j++)\n","\t\t\tA[i*nW+j] = 1.0f; //rand()%10;\n","\n","  // initialize mask\n","\tAvg_mask(M);\n","\n","\t/***********************************************************/\n","\t/*               convolution on host                       */\n","\t/***********************************************************/\n","\tdouble start = seconds();\n","\tconv2D_host(A, B, M);\n","\tdouble hostElaps = seconds() - start;\n","\n","\n","#if DEBUG\n","\t// print data\n","\tprintf(\"Print matrix A...\\n\");\n","\tfor (int i = 0; i < nH; i++) {\n","    if (i%8 == 0 && i>0)\n","      printf(\"\\n\");\n","\n","\t\tfor (int j = 0; j < nW; j++)\n","      if (j%8 == 0 && j>0)\n","\t\t\t  printf(\" %0.0f \", A[i*nW+j]);\n","      else\n","        printf(\"%0.0f \", A[i*nW+j]);\n","\t\tprintf(\"\\n\");\n","\t}\n","\n","\tprintf(\"Print matrix M ...\\n\");\n","\tfor (int i = 0; i < MASK_WIDTH; i++) {\n","\t\tfor (int j = 0; j < MASK_WIDTH; j++)\n","\t\t\t  printf(\" %1.2f \", M[i * MASK_WIDTH + j]);\n","\t\tprintf(\"\\n\");\n","\t}\n","\n","\t// print out data\n","\tprintf(\"Print results...\\n\");\n","\tfor (int i = 0; i < nH; i++) {\n","    if (i%8 == 0 && i>0)\n","      printf(\"\\n\");\n","\t\tfor (int j = 0; j < nW; j++)\n","      if (j%8 == 0 && j>0)\n","\t\t\t  printf(\" %0.2f \", B[i*nW+j]);\n","      else\n","        printf(\"%0.2f \", B[i*nW+j]);\n","\t\tprintf(\"\\n\");\n","\t}\n","#endif\n","\n","\n","\t/***********************************************************/\n","\t/*             convolution on device basic                 */\n","\t/***********************************************************/\n","\n","\t// cuda allocation\n","\tCHECK(cudaMemcpyToSymbol(M_dev, M, masksize));\n","\tCHECK(cudaMalloc((void **) &A_dev, datasize));\n","\tCHECK(cudaMalloc((void **) &B_dev, datasize));\n","\tCHECK(cudaMemcpy(A_dev, A, datasize, cudaMemcpyHostToDevice));\n","\n","\t// block, grid dims, kernel\n","\tdim3 block(b, b);\n","\tdim3 grid((nW+b-1)/b, (nH+b-1)/b);\n","\n","\tstart = seconds();\n","\tconv2D_basic<<<grid, block>>>(A_dev, B_dev);\n","  cudaDeviceSynchronize();\n","  double devElaps = seconds() - start;\n","\tprintf(\"\\nconv2D<<<(%d,%d), (%d,%d)>>> \\n\\n\", grid.x, grid.y, block.x, block.y);\n","\tCHECK(cudaGetLastError());\n","\n","\t/***********************************************************/\n","\t/*             convolution on device SMEM                  */\n","\t/***********************************************************/\n","\n","\tstart = seconds();\n","\tconv2D_SMEM<<<grid, block>>>(A_dev, B_dev);\n","  cudaDeviceSynchronize();\n","  double devElaps1 = seconds() - start;\n","\tprintf(\"\\nconv2D<<<(%d,%d), (%d,%d)>>> \\n\\n\", grid.x, grid.y, block.x, block.y);\n","\tCHECK(cudaGetLastError());\n","\n","\t// print exec times\n","\tprintf(\"Times:\\n\");\n","\tprintf(\"   - CPU elapsed time         = %f\\n\", hostElaps);\n","  printf(\"   - GPU elapsed time (SMEM)  = %f\\n\", devElaps1);\n","\tprintf(\"   - GPU elapsed time (basic) = %f\\n\", devElaps);\n","  printf(\"   - Speed-up (H/SMEM)        = %f\\n\", hostElaps / devElaps);\n","\tprintf(\"   - Speed-up (basic/SMEM)    = %f\\n\", devElaps / devElaps1);\n","\n","\tCHECK(cudaMemcpy(B, B_dev, datasize, cudaMemcpyDeviceToHost));\n","\n","#if DEBUG\n","\t// print out data\n","\tprintf(\"Print results...\\n\");\n","\tfor (int i = 0; i < nH; i++) {\n","    if (i%8 == 0 && i>0)\n","      printf(\"\\n\");\n","\t\tfor (int j = 0; j < nW; j++)\n","      if (j%8 == 0 && j>0)\n","\t\t\t  printf(\" %0.2f \", B[i*nW+j]);\n","      else\n","        printf(\"%0.2f \", B[i*nW+j]);\n","\t\tprintf(\"\\n\");\n","\t}\n","#endif\n","\n","\tcudaFree(A_dev);\n","\tcudaFree(B_dev);\n","  cudaDeviceReset();\n","\tfree(A);\n","\tfree(B);\n","\treturn 0;\n","}\n","\n","\n","void conv2D_host(float* A, float* B, const float* M) {\n","\t// find center position of kernel (half of kernel size)\n","\tint kCenterX = MASK_RADIUS;\n","\tint kCenterY = MASK_RADIUS;\n","\n","\tfor (int x = 0; x < DATA_WIDTH; x++)             // rows\n","\t\tfor (int y = 0; y < DATA_HEIGHT; y++) {        // columns\n","\n","\t\t\tfloat result = 0.0f;\n","\n","\t\t\t// start index on the input array\n","\t\t\tint startX = x - MASK_RADIUS;\n","\t\t\tint startY = y - MASK_RADIUS;\n","\n","\t\t\t// compute convolution\n","\t\t\tfor (int i = 0; i < MASK_WIDTH; ++i)\n","\t\t\t\tfor(int j = 0; j < MASK_WIDTH; j++ ) {\n","\t\t\t\t\tint X = startX + i;\n","\t\t\t\t\tint Y = startY + j;\n","\n","\t\t\t\t\t//boundary check\n","\t\t\t\t\tif((X >= 0) && (X < DATA_WIDTH) && (Y >= 0) && (Y < DATA_HEIGHT)) {\n","\t\t\t\t\t\tresult += A[(Y * DATA_WIDTH) + X] * M[(j * MASK_WIDTH) + i];\n","\t\t\t\t\t}\n","\t\t\t\t}\n","\n","\t\t\t//store final value\n","\t\t\tB[(y * DATA_WIDTH) + x] = result;\n","\t\t\t//printf(\"B[(%d * DATA_WIDTH) + %d] = %f\\n\",y, x, B[(y * DATA_WIDTH) + x]);\n","\t\t}\n","}\n","\n","// Average filter\n","void Avg_mask(float *mask) {\n","\tint n = MASK_WIDTH;\n","\tfor (int i = 0; i < n*n; i++)\n","\t\tmask[i] = (float) 1.0f / (n * n);\n","}"],"metadata":{"id":"ppbQ241n24g-"},"execution_count":null,"outputs":[]}]}