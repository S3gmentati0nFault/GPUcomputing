{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"collapsed_sections":["NO_C5o9-xRF_","iUYP4kCJhEIx","dA1aaC-9rmlI","lAVvcKOX_DU0"],"mount_file_id":"1mk0QXjREp-k_J-pdFfmgoC7-EVmgPyAo","authorship_tag":"ABX9TyMQycjrE8WGs4vhP0UB+fro"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["---\n","# **LAB 7 - CUDA Streams**\n","---"],"metadata":{"id":"fZYqN0UwVLC_"}},{"cell_type":"markdown","metadata":{"id":"NO_C5o9-xRF_"},"source":["# ▶️ CUDA setup"]},{"cell_type":"code","metadata":{"id":"8fekR2O4xRGE"},"source":["!nvcc --version"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Psl9iouxRGE"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["GPU computing notebooks download (from github)"],"metadata":{"id":"gcg1GyK5srek"}},{"cell_type":"code","source":["!git clone https://github.com/giulianogrossi/GPUcomputing.git"],"metadata":{"id":"tyHOxci3s3H8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"227eLdP5csN1"},"source":["NVCC Plugin for Jupyter notebook"]},{"cell_type":"code","source":["%cd GPUcomputing/utils/nvcc4jupyter-master/\n","!python3 -m build\n","%load_ext nvcc4jupyter\n","%cd /content/"],"metadata":{"id":"4TzxMBFds8aT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iUYP4kCJhEIx"},"source":["# ▶️ DeviceQuery"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kW9b_Yuxi7id","vscode":{"languageId":"python"}},"outputs":[],"source":["# DeviceQuery dell'attuale device (su Colab!)\n","!nvcc -arch=sm_75 /content/GPUcomputing/utils/deviceQuery.cu -o deviceQuery\n","!./deviceQuery"]},{"cell_type":"markdown","source":["# ✅ Streams and priority"],"metadata":{"id":"dA1aaC-9rmlI"}},{"cell_type":"markdown","source":["Simple stream..."],"metadata":{"id":"hm94SXqbZQBD"}},{"cell_type":"code","source":["%%cuda\n","\n","#include <stdio.h>\n","#include \"../../GPUcomputing/utils/common.h\"\n","\n","\n","// helper functions and utilities to work with CUDA\n","//#include <helper_functions.h>\n","//#include <helper_cuda.h>\n","\n","const char *sSDKsample = \"simpleStreams\";\n","\n","const char *sEventSyncMethod[] = {\"cudaEventDefault\",\n","                                  \"cudaEventBlockingSync\",\n","                                  \"cudaEventDisableTiming\", NULL};\n","\n","const char *sDeviceSyncMethod[] = {\"cudaDeviceScheduleAuto\",\n","                                   \"cudaDeviceScheduleSpin\",\n","                                   \"cudaDeviceScheduleYield\",\n","                                   \"INVALID\",\n","                                   \"cudaDeviceScheduleBlockingSync\", NULL};\n","\n","// kernel\n","__global__ void init_array(int *g_data, int *factor, int num_iterations) {\n","  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","  for (int i = 0; i < num_iterations; i++) {\n","    g_data[idx] += *factor;  // non-coalesced on purpose, to burn time\n","  }\n","}\n","\n","bool correct_data(int *a, const int n, const int c) {\n","  for (int i = 0; i < n; i++) {\n","    if (a[i] != c) {\n","      printf(\"%d: %d %d\\n\", i, a[i], c);\n","      return false;\n","    }\n","  }\n","\n","  return true;\n","}\n","\n","static const char *sSyncMethod[] = {\n","    \"0 (Automatic Blocking)\",\n","    \"1 (Spin Blocking)\",\n","    \"2 (Yield Blocking)\",\n","    \"3 (Undefined Blocking Method)\",\n","    \"4 (Blocking Sync Event) = low CPU utilization\",\n","    NULL};\n","\n","void printHelp() {\n","  printf(\"Usage: %s [options below]\\n\", sSDKsample);\n","  printf(\"\\t--sync_method=n for CPU/GPU synchronization\\n\");\n","  printf(\"\\t             n=%s\\n\", sSyncMethod[0]);\n","  printf(\"\\t             n=%s\\n\", sSyncMethod[1]);\n","  printf(\"\\t             n=%s\\n\", sSyncMethod[2]);\n","  printf(\"\\t   <Default> n=%s\\n\", sSyncMethod[4]);\n","  printf(\n","      \"\\t--use_generic_memory (default) use generic page-aligned for system \"\n","      \"memory\\n\");\n","  printf(\n","      \"\\t--use_cuda_malloc_host (optional) use cudaMallocHost to allocate \"\n","      \"system memory\\n\");\n","}\n","\n","/*******************************************************************************/\n","/*                                  MAIN\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t */\n","/*******************************************************************************/\n","int main(int argc, char **argv) {\n","  int nstreams = 4;              // number of streams for CUDA calls\n","  int nreps = 10;                // number of times each experiment is repeated\n","  int n = 16 * 1024 * 1024;      // number of ints in the data set\n","  int nbytes = n * sizeof(int);  // number of data bytes\n","  float elapsed_time, time_memcpy, time_kernel;  // timing variables\n","  int niterations = 5;  // number of iterations for the loop inside the kernel\n","\n","  // print dev features\n","  device_feat();\n","\n","  // allocate host memory\n","  int c = 5;            // value to which the array will be initialized\n","  int *h_a = 0;         // pointer to the array data in host memory\n","  CHECK(cudaMallocHost((void **)&h_a, nbytes));\n","\n","  // allocate device memory\n","  int *d_a = 0, *d_c = 0;  // pointers to data and init value in the device memory\n","  CHECK(cudaMalloc((void **)&d_a, nbytes));\n","  CHECK(cudaMemset(d_a, 0x0, nbytes));\n","  CHECK(cudaMalloc((void **)&d_c, sizeof(int)));\n","  CHECK(cudaMemcpy(d_c, &c, sizeof(int), cudaMemcpyHostToDevice));\n","\n","  printf(\"\\Operation\\ttime\\n\");\n","  printf(\"\\---------------------\\n\");\n","\n","  // allocate and initialize an array of stream handles\n","  cudaStream_t *streams = (cudaStream_t *)malloc(nstreams * sizeof(cudaStream_t));\n","\n","  for (int i = 0; i < nstreams; i++) {\n","    CHECK(cudaStreamCreate(&(streams[i])));\n","  }\n","\n","  // create CUDA event handles use blocking sync\n","  cudaEvent_t start_event, stop_event;\n","\n","  CHECK(cudaEventCreateWithFlags(&start_event, cudaEventDefault));\n","  CHECK(cudaEventCreateWithFlags(&stop_event, cudaEventDefault));\n","\n","  // time memcopy from device\n","  CHECK(cudaEventRecord(start_event, 0));  // record in stream-0, to\n","                                                     // ensure that all previous\n","                                                     // CUDA calls have\n","                                                     // completed\n","  CHECK(cudaMemcpyAsync(h_a, d_a, nbytes, cudaMemcpyDeviceToHost, streams[0]));\n","  CHECK(cudaEventRecord(stop_event, 0));\n","  CHECK(cudaEventSynchronize(stop_event));  // block until the event is actually recorded\n","  CHECK(cudaEventElapsedTime(&time_memcpy, start_event, stop_event));\n","  printf(\"memcopy:\\t%.2f\\n\", time_memcpy);\n","\n","  // time kernel\n","  dim3 threads = dim3(512, 1);\n","  dim3 blocks = dim3(n / threads.x, 1);\n","  CHECK(cudaEventRecord(start_event, 0));\n","  init_array<<<blocks, threads, 0, streams[0]>>>(d_a, d_c, niterations);\n","  CHECK(cudaEventRecord(stop_event, 0));\n","  CHECK(cudaEventSynchronize(stop_event));\n","  CHECK(cudaEventElapsedTime(&time_kernel, start_event, stop_event));\n","  printf(\"kernel:\\t\\t%.2f\\n\", time_kernel);\n","\n","  //********************************************\n","  // time non-streamed execution for reference\n","  //********************************************\n","  threads = dim3(512, 1);\n","  blocks = dim3(n / threads.x, 1);\n","  CHECK(cudaEventRecord(start_event, 0));\n","\n","  for (int k = 0; k < nreps; k++) {\n","    init_array<<<blocks, threads>>>(d_a, d_c, niterations);\n","    CHECK(cudaMemcpy(h_a, d_a, nbytes, cudaMemcpyDeviceToHost));\n","  }\n","\n","  CHECK(cudaEventRecord(stop_event, 0));\n","  CHECK(cudaEventSynchronize(stop_event));\n","  CHECK(cudaEventElapsedTime(&elapsed_time, start_event, stop_event));\n","  printf(\"non-streamed:\\t%.2f\\n\", elapsed_time / nreps);\n","\n","  //********************************************\n","  // time execution with nstreams streams\n","  //********************************************\n","  threads = dim3(512, 1);\n","  blocks = dim3(n / (nstreams * threads.x), 1);\n","  memset(h_a, 255, nbytes);           // set host memory bits to all 1s, for testing correctness\n","  CHECK(cudaMemset(d_a, 0, nbytes));  // set device memory to all 0s, for testing correctness\n","  CHECK(cudaEventRecord(start_event, 0));\n","\n","  for (int k = 0; k < nreps; k++) {\n","    // asynchronously launch nstreams kernels, each operating on its own portion of data\n","    for (int i = 0; i < nstreams; i++) {\n","      init_array<<<blocks, threads, 0, streams[i]>>>(d_a + i * n / nstreams, d_c, niterations);\n","    }\n","\n","    // asynchronously launch nstreams memcopies.  Note that memcopy in stream x will only\n","    // commence executing when all previous CUDA calls in stream x have completed\n","    for (int i = 0; i < nstreams; i++) {\n","      CHECK(cudaMemcpyAsync(h_a + i * n / nstreams,\n","                                      d_a + i * n / nstreams, nbytes / nstreams,\n","                                      cudaMemcpyDeviceToHost, streams[i]));\n","    }\n","  }\n","\n","  CHECK(cudaEventRecord(stop_event, 0));\n","  CHECK(cudaEventSynchronize(stop_event));\n","  CHECK(cudaEventElapsedTime(&elapsed_time, start_event, stop_event));\n","  printf(\"%d streams:\\t%.2f\\n\", nstreams, elapsed_time / nreps);\n","\n","  // check whether the output is correct\n","  printf(\"---------------------\\n\");\n","  bool bResults = correct_data(h_a, n, c * nreps * niterations);\n","\n","  // release resources\n","  for (int i = 0; i < nstreams; i++) {\n","    CHECK(cudaStreamDestroy(streams[i]));\n","  }\n","\n","  CHECK(cudaEventDestroy(start_event));\n","  CHECK(cudaEventDestroy(stop_event));\n","\n","  // Free cudaMallocHost\n","  cudaFreeHost(h_a);\n","\n","  CHECK(cudaFree(d_a));\n","  CHECK(cudaFree(d_c));\n","\n","  return bResults ? EXIT_SUCCESS : EXIT_FAILURE;\n","}"],"metadata":{"id":"Ru14xyOKrpIV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Stream priorities..."],"metadata":{"id":"638D3qVhU6zm"}},{"cell_type":"code","source":["%%cuda\n","#include <cstdio>\n","#include <stdio.h>\n","#include \"../../GPUcomputing/utils/common.h\"\n","\n","#define TOTAL_SIZE 256 * 1024 * 1024\n","#define EACH_SIZE 128 * 1024 * 1024\n","\n","// threadblocks\n","#define TBLOCKS 1024\n","#define THREADS 512\n","\n","\n","// copy from source -> destination arrays\n","__global__ void memcpy_kernel(int *dst, int *src, size_t n) {\n","  int num = gridDim.x * blockDim.x;\n","  int id = blockDim.x * blockIdx.x + threadIdx.x;\n","\n","  for (int i = id; i < n / sizeof(int); i += num) {\n","    dst[i] = src[i];\n","  }\n","}\n","\n","// initialise memory\n","void mem_init(int *buf, size_t n) {\n","  for (int i = 0; i < n / sizeof(int); i++) {\n","    buf[i] = i;\n","  }\n","}\n","\n","/******************************************************************************/\n","/*                                  MAIN\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t*/\n","/******************************************************************************/\n","int main(int argc, char **argv) {\n","  cudaDeviceProp device_prop;\n","  int dev_id;\n","\n","  printf(\"Starting...\\n\");\n","\n","  // set device\n","  dev_id = findCudaDevice(argc, (const char **)argv);\n","  CHECK(cudaGetDeviceProperties(&device_prop, dev_id));\n","\n","  // get the range of priorities available\n","  // [ greatest_priority, lowest_priority ]\n","  int priority_low;\n","  int priority_hi;\n","  CHECK(cudaDeviceGetStreamPriorityRange(&priority_low, &priority_hi));\n","\n","  printf(\"CUDA stream priority range: LOW: %d to HIGH: %d\\n\", priority_low, priority_hi);\n","\n","  // create streams with highest and lowest available priorities\n","  cudaStream_t st_low;\n","  cudaStream_t st_hi;\n","  CHECK(cudaStreamCreateWithPriority(&st_low, cudaStreamNonBlocking, priority_low));\n","  CHECK(cudaStreamCreateWithPriority(&st_hi, cudaStreamNonBlocking, priority_hi));\n","\n","  size_t size;\n","  size = TOTAL_SIZE;\n","\n","  // initialise host data\n","  int *h_src_low;\n","  int *h_src_hi;\n","  h_src_low = (int *)malloc(size);\n","  h_src_hi = (int *)malloc(size);\n","  mem_init(h_src_low, size);\n","  mem_init(h_src_hi, size);\n","\n","  // initialise device data\n","  int *h_dst_low;\n","  int *h_dst_hi;\n","  h_dst_low = (int *)malloc(size);\n","  h_dst_hi = (int *)malloc(size);\n","  memset(h_dst_low, 0, size);\n","  memset(h_dst_hi, 0, size);\n","\n","  // copy source data -> device\n","  int *d_src_low;\n","  int *d_src_hi;\n","  CHECK(cudaMalloc(&d_src_low, size));\n","  CHECK(cudaMalloc(&d_src_hi, size));\n","  CHECK(cudaMemcpy(d_src_low, h_src_low, size, cudaMemcpyHostToDevice));\n","  CHECK(cudaMemcpy(d_src_hi, h_src_hi, size, cudaMemcpyHostToDevice));\n","\n","  // allocate memory for memcopy destination\n","  int *d_dst_low;\n","  int *d_dst_hi;\n","  CHECK(cudaMalloc(&d_dst_low, size));\n","  CHECK(cudaMalloc(&d_dst_hi, size));\n","\n","  // create some events\n","  cudaEvent_t ev_start_low;\n","  cudaEvent_t ev_start_hi;\n","  cudaEvent_t ev_end_low;\n","  cudaEvent_t ev_end_hi;\n","  CHECK(cudaEventCreate(&ev_start_low));\n","  CHECK(cudaEventCreate(&ev_start_hi));\n","  CHECK(cudaEventCreate(&ev_end_low));\n","  CHECK(cudaEventCreate(&ev_end_hi));\n","\n","  /* */\n","\n","  // call pair of kernels repeatedly (with different priority streams)\n","  CHECK(cudaEventRecord(ev_start_low, st_low));\n","  CHECK(cudaEventRecord(ev_start_hi, st_hi));\n","\n","  for (int i = 0; i < TOTAL_SIZE; i += EACH_SIZE) {\n","    int j = i / sizeof(int);\n","    memcpy_kernel<<<TBLOCKS, THREADS, 0, st_low>>>(d_dst_low + j, d_src_low + j, EACH_SIZE);\n","    memcpy_kernel<<<TBLOCKS, THREADS, 0, st_hi>>>(d_dst_hi + j, d_src_hi + j, EACH_SIZE);\n","  }\n","\n","  CHECK(cudaEventRecord(ev_end_low, st_low));\n","  CHECK(cudaEventRecord(ev_end_hi, st_hi));\n","\n","  CHECK(cudaEventSynchronize(ev_end_low));\n","  CHECK(cudaEventSynchronize(ev_end_hi));\n","\n","  /* */\n","\n","  size = TOTAL_SIZE;\n","  CHECK(cudaMemcpy(h_dst_low, d_dst_low, size, cudaMemcpyDeviceToHost));\n","  CHECK(cudaMemcpy(h_dst_hi, d_dst_hi, size, cudaMemcpyDeviceToHost));\n","\n","  // check results of kernels\n","  memcmp(h_dst_low, h_src_low, size);\n","  memcmp(h_dst_hi, h_src_hi, size);\n","\n","  // check timings\n","  float ms_low;\n","  float ms_hi;\n","  CHECK(cudaEventElapsedTime(&ms_low, ev_start_low, ev_end_low));\n","  CHECK(cudaEventElapsedTime(&ms_hi, ev_start_hi, ev_end_hi));\n","\n","  printf(\"elapsed time of kernels launched to LOW priority stream: %.3lf ms\\n\", ms_low);\n","  printf(\"elapsed time of kernels launched to HI  priority stream: %.3lf ms\\n\", ms_hi);\n","\n","  exit(EXIT_SUCCESS);\n","}"],"metadata":{"id":"krfu8JpzU_Ty"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lAVvcKOX_DU0"},"source":["# ✅ Somma array con stream"]},{"cell_type":"markdown","source":["This example demonstrates overlapping computation and communication by\n","partitioning a data set and asynchronously launching the memory copies and kernels for each subset. Launching all transfers and kernels for a given subset in the same CUDA stream ensures that computation on the device is not started until the necessary data has been transferred. However, because the work of each subset is independent of all other subsets, the communication and computation of different subsets will overlap.\n","\n","This example launches copies and kernels in breadth-first order."],"metadata":{"id":"xpJ_WqJ-kJfy"}},{"cell_type":"code","metadata":{"id":"OH6TuWnB-_0M"},"source":["%%cuda\n","\n","#include \"../../GPUcomputing/utils/common.h\"\n","\n","#define NSTREAM 4\n","#define BDIM 128\n","\n","void initialData(float *ip, int size) {\n","  int i;\n","\n","  for(i = 0; i < size; i++)\n","    ip[i] = (float)(rand() & 0xFF) / 10.0f;\n","}\n","\n","void sumArraysOnHost(float *A, float *B, float *C, const int N) {\n","  for (int idx = 0; idx < N; idx++)\n","    C[idx] = A[idx] + B[idx];\n","}\n","\n","__global__ void sumArrays(float *A, float *B, float *C, const int N) {\n","  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","  if (idx < N)\n","    C[idx] = A[idx] + B[idx];\n","}\n","\n","void checkResult(float *hostRef, float *gpuRef, const int N) {\n","  double epsilon = 1.0E-8;\n","  bool match = 1;\n","\n","  for (int i = 0; i < N; i++) {\n","    if (abs(hostRef[i] - gpuRef[i]) > epsilon) {\n","      match = 0;\n","      printf(\"Arrays do not match!\\n\");\n","      printf(\"host %5.2f gpu %5.2f at %d\\n\", hostRef[i], gpuRef[i], i);\n","      break;\n","    }\n","  }\n","  if (match)\n","    printf(\"Arrays match.\\n\\n\");\n","}\n","\n","/******************************************************************************/\n","/*                                  MAIN\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t*/\n","/******************************************************************************/\n","int main(int argc, char **argv) {\n","  printf(\"> %s Starting...\\n\", argv[0]);\n","\n","  int dev = 0;\n","  cudaDeviceProp deviceProp;\n","  CHECK(cudaGetDeviceProperties(&deviceProp, dev));\n","  printf(\"> Using Device %d: %s\\n\", dev, deviceProp.name);\n","  CHECK(cudaSetDevice(dev));\n","\n","  // check if device support hyper-q\n","  if (deviceProp.major < 3 || (deviceProp.major == 3 && deviceProp.minor < 5)) {\n","    if (deviceProp.concurrentKernels == 0) {\n","      printf(\"> GPU does not support concurrent kernel execution (SM 3.5 or higher required)\\n\");\n","      printf(\"> CUDA kernel runs will be serialized\\n\");\n","    }\n","    else {\n","      printf(\"> GPU does not support HyperQ\\n\");\n","      printf(\"> CUDA kernel runs will have limited concurrency\\n\");\n","    }\n","  }\n","\n","  // Shows whether the device can transfer in both directions simultaneously\n","  printf(\"> Device %s capable of simultaneous CPU-to-GPU datatransfers\\n\", deviceProp.deviceOverlap ? \"IS\": \"NOT\");\n","\n","  printf(\"> Compute Capability %d.%d hardware with %d multi-processors\\n\",\n","          deviceProp.major, deviceProp.minor, deviceProp.multiProcessorCount);\n","\n","  printf (\"> with streams = %d\\n\", NSTREAM);\n","\n","  // set up data size of vectors\n","  int nElem = 1 << 26;\n","  printf(\"> vector size = %d\\n\", nElem);\n","  size_t nBytes = nElem * sizeof(float);\n","\n","  // malloc pinned host memory for async memcpy\n","  float *h_A, *h_B, *hostRef, *gpuRef;\n","  CHECK(cudaHostAlloc((void**)&h_A, nBytes, cudaHostAllocDefault));\n","  CHECK(cudaHostAlloc((void**)&h_B, nBytes, cudaHostAllocDefault));\n","  CHECK(cudaHostAlloc((void**)&gpuRef, nBytes, cudaHostAllocDefault));\n","  CHECK(cudaHostAlloc((void**)&hostRef, nBytes, cudaHostAllocDefault));\n","\n","  // initialize data at host side\n","  initialData(h_A, nElem);\n","  initialData(h_B, nElem);\n","  memset(hostRef, 0, nBytes);\n","  memset(gpuRef,  0, nBytes);\n","\n","  // add vector at host side for result checks\n","  sumArraysOnHost(h_A, h_B, hostRef, nElem);\n","\n","  // malloc device global memory\n","  float *d_A, *d_B, *d_C;\n","  CHECK(cudaMalloc((float**)&d_A, nBytes));\n","  CHECK(cudaMalloc((float**)&d_B, nBytes));\n","  CHECK(cudaMalloc((float**)&d_C, nBytes));\n","\n","  cudaEvent_t start, stop;\n","  CHECK(cudaEventCreate(&start));\n","  CHECK(cudaEventCreate(&stop));\n","\n","  // invoke kernel at host side\n","  dim3 block (BDIM);\n","  dim3 grid  ((nElem + block.x - 1) / block.x);\n","  printf(\"> grid (%d, %d) block (%d, %d)\\n\", grid.x, grid.y, block.x, block.y);\n","\n","  // sequential operation\n","  CHECK(cudaEventRecord(start, 0));\n","  CHECK(cudaMemcpy(d_A, h_A, nBytes, cudaMemcpyHostToDevice));\n","  CHECK(cudaMemcpy(d_B, h_B, nBytes, cudaMemcpyHostToDevice));\n","  CHECK(cudaEventRecord(stop, 0));\n","  CHECK(cudaEventSynchronize(stop));\n","  float memcpy_h2d_time;\n","  CHECK(cudaEventElapsedTime(&memcpy_h2d_time, start, stop));\n","\n","  CHECK(cudaEventRecord(start, 0));\n","  sumArrays<<<grid, block>>>(d_A, d_B, d_C, nElem);\n","  CHECK(cudaEventRecord(stop, 0));\n","  CHECK(cudaEventSynchronize(stop));\n","  float kernel_time;\n","  CHECK(cudaEventElapsedTime(&kernel_time, start, stop));\n","\n","  CHECK(cudaEventRecord(start, 0));\n","  CHECK(cudaMemcpy(gpuRef, d_C, nBytes, cudaMemcpyDeviceToHost));\n","  CHECK(cudaEventRecord(stop, 0));\n","  CHECK(cudaEventSynchronize(stop));\n","  float memcpy_d2h_time;\n","  CHECK(cudaEventElapsedTime(&memcpy_d2h_time, start, stop));\n","  float itotal = kernel_time + memcpy_h2d_time + memcpy_d2h_time;\n","\n","  printf(\"\\n\");\n","  printf(\"Measured timings (throughput):\\n\");\n","  printf(\" Memcpy host to device\\t: %f ms (%f GB/s)\\n\", memcpy_h2d_time, (nBytes * 1e-6) / memcpy_h2d_time);\n","  printf(\" Memcpy device to host\\t: %f ms (%f GB/s)\\n\", memcpy_d2h_time, (nBytes * 1e-6) / memcpy_d2h_time);\n","  printf(\" Kernel\\t\\t\\t: %f ms (%f GB/s)\\n\", kernel_time, (nBytes * 2e-6) / kernel_time);\n","  printf(\" Total\\t\\t\\t: %f ms (%f GB/s)\\n\", itotal, (nBytes * 2e-6) / itotal);\n","\n","  // grid parallel operation\n","  int iElem = nElem / NSTREAM;\n","  size_t iBytes = iElem * sizeof(float);\n","  grid.x = (iElem + block.x - 1) / block.x;\n","\n","  cudaStream_t stream[NSTREAM];\n","\n","  for (int i = 0; i < NSTREAM; ++i)\n","    CHECK(cudaStreamCreate(&stream[i]));\n","\n","  CHECK(cudaEventRecord(start, 0));\n","\n","  // initiate all asynchronous transfers to the device\n","  for (int i = 0; i < NSTREAM; ++i) {\n","    int ioffset = i * iElem;\n","    CHECK(cudaMemcpyAsync(&d_A[ioffset], &h_A[ioffset], iBytes, cudaMemcpyHostToDevice, stream[i]));\n","    CHECK(cudaMemcpyAsync(&d_B[ioffset], &h_B[ioffset], iBytes, cudaMemcpyHostToDevice, stream[i]));\n","  }\n","\n","  // launch a kernel in each stream\n","  for (int i = 0; i < NSTREAM; ++i) {\n","    int ioffset = i * iElem;\n","    sumArrays<<<grid, block, 0, stream[i]>>>(&d_A[ioffset], &d_B[ioffset], &d_C[ioffset], iElem);\n","  }\n","\n","  // enqueue asynchronous transfers from the device\n","  for (int i = 0; i < NSTREAM; ++i) {\n","    int ioffset = i * iElem;\n","    CHECK(cudaMemcpyAsync(&gpuRef[ioffset], &d_C[ioffset], iBytes, cudaMemcpyDeviceToHost, stream[i]));\n","  }\n","\n","  CHECK(cudaEventRecord(stop, 0));\n","  CHECK(cudaEventSynchronize(stop));\n","  float execution_time;\n","  CHECK(cudaEventElapsedTime(&execution_time, start, stop));\n","\n","  printf(\"\\n\");\n","  printf(\"Actual results from overlapped data transfers:\\n\");\n","  printf(\" overlap with %d streams : %f ms (%f GB/s)\\n\", NSTREAM, execution_time, (nBytes * 2e-6) / execution_time );\n","  printf(\" speedup                : %f \\n\", itotal/execution_time);\n","\n","  // check kernel error\n","  CHECK(cudaGetLastError());\n","\n","  // check device results\n","  checkResult(hostRef, gpuRef, nElem);\n","\n","  // free device global memory\n","  CHECK(cudaFree(d_A));\n","  CHECK(cudaFree(d_B));\n","  CHECK(cudaFree(d_C));\n","\n","  // free host memory\n","  CHECK(cudaFreeHost(h_A));\n","  CHECK(cudaFreeHost(h_B));\n","  CHECK(cudaFreeHost(hostRef));\n","  CHECK(cudaFreeHost(gpuRef));\n","\n","  // destroy events\n","  CHECK(cudaEventDestroy(start));\n","  CHECK(cudaEventDestroy(stop));\n","\n","  // destroy streams\n","  for (int i = 0; i < NSTREAM; ++i)\n","    CHECK(cudaStreamDestroy(stream[i]));\n","\n","  CHECK(cudaDeviceReset());\n","  return(0);\n","}\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vXUIQkZLCTcG"},"source":["# ✅ Tabular\n"]},{"cell_type":"markdown","source":["### ↘️ *`TODO...`*"],"metadata":{"id":"eRE_juKbwT37"}},{"cell_type":"markdown","source":["1. Come modificare il kernel per usare gli stream\n","2. Gestione della memoria pinned e device\n","\n","Applicare\n","3. Schema: loop over {copy, kernel, copy}\n","4. Schema: loop over {copy H2D}, loop over {kernel}, loop over {copy D2H}\n"],"metadata":{"id":"NtEOIgt3utGz"}},{"cell_type":"code","metadata":{"id":"-Y52R0d3CA50"},"source":["%%cuda\n","\n","#include <stdio.h>\n","#include \"../../GPUcomputing/utils/common.h\"\n","\n","#define PI 3.141592f\n","\n","/*\n"," * Kernel: tabular function\n"," */\n","__global__ void tabular(float *a, int n) {\n","\tint i = threadIdx.x + blockIdx.x * blockDim.x;\n","\tif (i < n) {\n","\t\tfloat x = PI * (float)i / (float)n;\n","\t\tfloat s = sinf(x);\n","\t\tfloat c = cosf(x);\n","\t\ta[i] = sqrtf(abs(s * s - c * c));\n","\t}\n","}\n","\n","/*\n"," * Kernel: tabular function using streams\n"," */\n","__global__ void tabular_streams(float *a, int n, int offset) {\n","\tint i = offset + threadIdx.x + blockIdx.x * blockDim.x;\n","  if (i < n) {\n","    float x = PI * (float)i / (float)n;\n","    float s = sinf(x);\n","    float c = cosf(x);\n","    a[i] = sqrtf(abs(s * s - c * c));\n","  }\n","}\n","\n","\n","/******************************************************************************/\n","/*                                  MAIN\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t*/\n","/******************************************************************************/\n","int main(void) {\n","\n","  // main params\n","  uint MB = 1024*1024;\n","  uint n = 256*MB;\n","\tint blockSize = 256;\n","\tint nStreams = 8;\n","\n","\tint streamSize = n / nStreams;\n","\tint streamBytes = streamSize * sizeof(float);\n","\tint bytes = n * sizeof(float);\n","\n","\tint devId = 0;\n","\tcudaDeviceProp prop;\n","\tCHECK(cudaGetDeviceProperties(&prop, devId));\n","\tprintf(\"Device : %s\\n\\n\", prop.name);\n","\tCHECK(cudaSetDevice(devId));\n","  printf(\"Array size   : %d\\n\", n);\n","  printf(\"StreamSize   : %d\\n\", streamSize);\n","  printf(\"Memory bytes : %d (MB)\\n\", bytes/MB);\n","  printf(\"streamBytes  : %d (MB)\\n\", streamBytes/MB);\n","\n","\t// allocate pinned host memory and device memory\n","\tfloat *a, *d_a;\n","\tCHECK(cudaMallocHost((void**) &a, bytes));      // host pinned\n","\tCHECK(cudaMalloc((void**) &d_a, bytes));        // device\n","\n","\tfloat ms; // elapsed time in milliseconds\n","\n","\t// create events and streams\n","\tcudaEvent_t startEvent, stopEvent, dummyEvent;\n","\tcudaStream_t stream[nStreams];\n","\tCHECK(cudaEventCreate(&startEvent));\n","\tCHECK(cudaEventCreate(&stopEvent));\n","\tCHECK(cudaEventCreate(&dummyEvent));\n","\tfor (int i = 0; i < nStreams; ++i)\n","\t\tCHECK(cudaStreamCreate(&stream[i]));\n","\n","\t/******************************************************************************/\n","\t/*       baseline case - sequential transfer and execute      \t\t\t\t\t\t\t  */\n","\t/******************************************************************************/\n","\tmemset(a, 0, bytes);\n","\tCHECK(cudaEventRecord(startEvent, 0));\n","\tCHECK(cudaMemcpy(d_a, a, bytes, cudaMemcpyHostToDevice));\n","\ttabular<<<n / blockSize, blockSize>>>(d_a, n);\n","\tCHECK(cudaMemcpy(a, d_a, bytes, cudaMemcpyDeviceToHost));\n","\tCHECK(cudaEventRecord(stopEvent, 0));\n","\tCHECK(cudaEventSynchronize(stopEvent));\n","\tCHECK(cudaEventElapsedTime(&ms, startEvent, stopEvent));\n","\tprintf(\"\\nTime for sequential transfer and execute (ms): %f\\n\", ms);\n","\n","  /******************************************************************************/\n","\t/*       asynchronous version 1: loop over {copy, kernel, copy}\t\t\t\t\t\t\t  */\n","\t/******************************************************************************/\n","\tmemset(a, 0, bytes);\n","\tCHECK(cudaEventRecord(startEvent, 0));\n","\tfor (int i = 0; i < nStreams; ++i) {\n","\t\tint offset = i * streamSize;\n","\t\tCHECK(cudaMemcpyAsync(&d_a[offset], &a[offset], streamBytes,cudaMemcpyHostToDevice, stream[i]));\n","\t\ttabular_streams<<<streamSize / blockSize, blockSize, 0, stream[i]>>>(d_a, n, offset);\n","\t\tCHECK(cudaMemcpyAsync(&a[offset], &d_a[offset], streamBytes, cudaMemcpyDeviceToHost, stream[i]));\n","\t}\n","\tCHECK(cudaEventRecord(stopEvent, 0));\n","\tCHECK(cudaEventSynchronize(stopEvent));\n","\tCHECK(cudaEventElapsedTime(&ms, startEvent, stopEvent));\n","\tprintf(\"\\nTime for asynchronous loop over {copy, kernel, copy} transfer and execute (ms): %f\\n\", ms);\n","\n","\n","  /******************************************************************************/\n","\t/* asynchronous version 2: loop over copy, loop over kernel, loop over copy\t  */\n","\t/******************************************************************************/\n","\n","  memset(a, 0, bytes);\n","\tCHECK(cudaEventRecord(startEvent, 0));\n","\tfor (int i = 0; i < nStreams; ++i) {\n","\t\tint offset = i * streamSize;\n","\t\tCHECK(cudaMemcpyAsync(&d_a[offset], &a[offset], streamBytes,cudaMemcpyHostToDevice, stream[i]));\n","\t}\n","\tfor (int i = 0; i < nStreams; ++i) {\n","\t\tint offset = i * streamSize;\n","\t\ttabular_streams<<<streamSize / blockSize, blockSize, 0, stream[i]>>>(d_a,n,offset);\n","\t}\n","\tfor (int i = 0; i < nStreams; ++i) {\n","\t\tint offset = i * streamSize;\n","\t\tCHECK(cudaMemcpyAsync(&a[offset], &d_a[offset], streamBytes,cudaMemcpyDeviceToHost, stream[i]));\n","\t}\n","\tCHECK(cudaEventRecord(stopEvent, 0));\n","\tCHECK(cudaEventSynchronize(stopEvent));\n","\tCHECK(cudaEventElapsedTime(&ms, startEvent, stopEvent));\n","\tprintf(\"\\nTime for asynchronous loop over copy, loop over kernel, loop over copy transfer and execute (ms): %f\\n\", ms);\n","\n","\t// cleanup\n","\tCHECK(cudaEventDestroy(startEvent));\n","\tCHECK(cudaEventDestroy(stopEvent));\n","\tCHECK(cudaEventDestroy(dummyEvent));\n","\tfor (int i = 0; i < nStreams; ++i)\n","\t\tCHECK(cudaStreamDestroy(stream[i]));\n","\tcudaFree(d_a);\n","\tcudaFreeHost(a);\n","\n","\treturn 0;\n","}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0PSc9B9PDTWt"},"source":["# Compilazione ed esecuzione\n","\n","!nvcc -arch=sm_75 src/tmp/single_file.cu  -o tabular\n","!./tabular"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mfR471rze2p-"},"source":["# profilazione\n","\n","!nvprof ./tabular"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SOFMQZAkjlLW"},"source":["# ✅ MQDB con stream"]},{"cell_type":"markdown","source":["### ↘️ *`TODO...`*"],"metadata":{"id":"zBQwv6uAkqEv"}},{"cell_type":"markdown","source":["Disegnare un kernel per il prodotto tra matrici MQDB con le seguenti specifiche:\n","- Allocare spazio per matrici MQDB su CPU e GPU\n","- Confrontare uso di memoria unificata vs memoria asincrona\n","- Introdurre gli stream su cui distribuire il carico (grid parall.)\n","- Analisi di prestazioni usando i tempi ricavati con CUDA event"],"metadata":{"id":"6yomfFJusrYT"}},{"cell_type":"code","metadata":{"id":"v9nRkLgeB10A"},"source":["%%cuda_group_save --name \"MQDB_stream_Unified.cu\" --group \"STREAMS\"\n","\n","\n","#include \"../../GPUcomputing/utils/MQDB/mqdb.h\"\n","#include \"../../GPUcomputing/utils/common.h\"\n","\n","#define BLOCK_SIZE 16     // block size\n","#define TEST_CPU 0\n","\n","/*\n"," * Kernel for standard (naive) matrix product\n"," */\n","__global__ void matProdKernel(mqdb *A, mqdb *B, mqdb *C, int n) {\n","\t// row & col indexes\n","\tint row = blockIdx.y * blockDim.y + threadIdx.y;\n","\tint col = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","\t// each thread computes an entry of the product matrix\n","\tif ((row < n) && (col < n)) {\n","\t\tfloat val = 0;\n","\t\tfor (int k = 0; k < n; k++)\n","\t\t\tval += A->elem[row * n + k] * B->elem[k * n + col];\n","\t\tC->elem[row * n + col] = val;\n","\t}\n","}\n","\n","/*\n"," * Kernel for block sub-matrix product of mqdb\n"," */\n","__global__ void mqdbBlockProd(mqdb *A, mqdb *B, mqdb *C, uint sdim, uint d, uint n) {\n","\tint row = blockIdx.y * blockDim.y + threadIdx.y;\n","\tint col = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","\t// jump to the right block sub-matrix\n","\tuint  offset = (n+1)*sdim;\n","\n","\t// each thread computes an entry of the product matrix\n","\tif ((row < d) && (col < d)) {\n","\t\tfloat val = 0;\n","\t\tfor (int k = 0; k < d; k++)\n","\t\t\tval += A->elem[row * n + k + offset] * B->elem[k * n + col + offset];\n","\t\tC->elem[row * n + col + offset] = val;\n","\t}\n","}\n","\n","\n","/*\n"," * Test on MQDB kernels using Unified Memory\n"," */\n","void testKernelsMQDB_unified(uint n, uint k, cudaEvent_t start, cudaEvent_t stop) {\n","\n","\t// matrix instance generation - Unified Memory\n","\tmqdb *A, *B, *C;\n","\tCHECK(cudaMallocManaged(&A, sizeof(mqdb)));\n","  CHECK(cudaMallocManaged(&A->blkSize, k*sizeof(int)));\n","  CHECK(cudaMallocManaged(&A->elem, n*n*sizeof(float)));\n","\n","  CHECK(cudaMallocManaged(&B, sizeof(mqdb)));\n","  CHECK(cudaMallocManaged(&B->blkSize, k*sizeof(int)));\n","  CHECK(cudaMallocManaged(&B->elem, n*n*sizeof(float)));\n","\n","  CHECK(cudaMallocManaged(&C, sizeof(mqdb)));\n","  CHECK(cudaMallocManaged(&C->blkSize, k*sizeof(int)));\n","  CHECK(cudaMallocManaged(&C->elem, n*n*sizeof(float)));\n","\n","  // random fill mat entries\n","  int seed = 1;\n","\tgenRandDimsUnified(A, n, k, seed);\n","\tgenRandDimsUnified(B, n, k, seed);\n","\tgenRandDimsUnified(C, n, k, seed);\n","\tfillBlocksUnified(A, n, k, 'C', 1);\n","\tfillBlocksUnified(B, n, k, 'C', 2);\n","\tfillBlocksUnified(C, n, k, 'C', 0);\n","\n","\tulong nBytes = n * n * sizeof(float);\n","\tprintf(\"Memory size required = %3.4f (MB)\\n\",(float)nBytes/(1024.0*1024.0));\n","\n","\n","\t/***********************************************************/\n","\t/*                     GPU mat product                     */\n","\t/***********************************************************/\n","\n","  printf(\"Kernel (naive) mat product...\\n\");\n","\tdim3 block(BLOCK_SIZE, BLOCK_SIZE);\n","\tdim3 grid((n + block.x - 1) / block.x, (n + block.y - 1) / block.y);\n","  float milliseconds;\n","\tCHECK(cudaEventRecord(start));\n","\tmatProdKernel<<<grid, block>>>(A, B, C, n);\n","  CHECK(cudaDeviceSynchronize());\n","\tCHECK(cudaEventRecord(stop));\n","\tCHECK(cudaEventSynchronize(stop));\n","\tCHECK(cudaEventElapsedTime(&milliseconds, start, stop));\n","\tfloat GPUtime1 = milliseconds / 1000.0;\n","\tprintf(\"   elapsed time               : %.4f (sec)\\n\", GPUtime1);\n","\t//printf(\"   speedup vs CPU MQDB product: %.2f\\n\\n\", CPUtime/GPUtime1);\n","\t//mqdbDisplay(C);\n","\n","\t/***********************************************************/\n","\t/*                     GPU MQDB product                    */\n","\t/***********************************************************/\n","\n","  printf(\"Kernel MQDB product...\\n\");\n","\tuint sdim = 0;\n","\tCHECK(cudaEventRecord(start));\n","\tfor (uint i = 0; i < k; i++ ) {\n","\t\tuint d = A->blkSize[i];\n","\t\tmqdbBlockProd<<<grid, block>>>(A, B, C, sdim, d, n);\n","\t\tsdim += d;\n","\t}\n","\tCHECK(cudaDeviceSynchronize());\n","\tCHECK(cudaEventRecord(stop));\n","\tCHECK(cudaEventSynchronize(stop));\n","\tCHECK(cudaEventElapsedTime(&milliseconds, start, stop));\n","\tfloat GPUtime2 = milliseconds / 1000.0;\n","\tprintf(\"   elapsed time                  :  %.4f (sec)\\n\", GPUtime2);\n","\t//printf(\"   speedup vs CPU MQDB product   :  %.2f\\n\", CPUtime/GPUtime2);\n","\tprintf(\"   speedup vs GPU std mat product:  %.4f\\n\\n\", GPUtime1/GPUtime2);\n","\n","  /***********************************************************/\n","\t/*             GPU MQDB product using streams              */\n","\t/***********************************************************/\n","\n","  printf(\"Kernel MQDB product using streams...\\n\");\n","\n","  // create and use streams\n","\tint nstreams = A->nBlocks;\n","\tcudaStream_t streams[nstreams];\n","\tfor (int i = 0; i < nstreams; i++)\n","\t\tCHECK(cudaStreamCreate(&streams[i]));\n","\n","\tuint dsum = 0;  // bound dx\n","\tCHECK(cudaEventRecord(start));\n","\tfor (int i = 0; i < nstreams; i++) {\n","\t\tuint d = A->blkSize[i];\n","\t\tdim3 grid((d + block.x - 1) / block.x, (d + block.y - 1) / block.y);\n","\t\tmqdbBlockProd<<<grid, block, 0, streams[i]>>>(A, B, C, dsum, d, n);\n","\t\tdsum += d;\n","\t}\n","\tCHECK(cudaDeviceSynchronize());\n","\tCHECK(cudaEventRecord(stop));\n","\tCHECK(cudaEventSynchronize(stop));\n","\tCHECK(cudaEventElapsedTime(&milliseconds, start, stop));\n","\tfloat GPUtime3 = milliseconds / 1000.0;\n","\tprintf(\"   elapsed time                  : %.5f (sec)\\n\", GPUtime3);\n","\t//printf(\"   speedup vs CPU MQDB product   : %.2f\\n\", CPUtime/GPUtime3);\n","\tprintf(\"   speedup vs GPU std mat product: %.2f\\n\",GPUtime1/GPUtime3);\n","  printf(\"   speedup vs GPU MQDB product   : %.2f\\n\",GPUtime2/GPUtime3);\n","  //mqdbDisplay(C);\n","\n","\t// clean up streams and events\n","\tfor (int i = 0; i < nstreams; i++)\n","\t\tcudaStreamDestroy(streams[i]);\n","\n","}\n","\n","/******************************************************************************/\n","/*                                  MAIN\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t*/\n","/******************************************************************************/\n","\n","int main(int argc, char *argv[]) {\n","\n","  // set up device\n","\tint dev = 0;\n","\tcudaDeviceProp deviceProp;\n","\tCHECK(cudaGetDeviceProperties(&deviceProp, dev));\n","\tprintf(\"%s starting mqdb product at \", argv[0]);\n","\tprintf(\"device %d: %s\\n\", dev, deviceProp.name);\n","\tCHECK(cudaSetDevice(dev));\n","\n","\t// events to measure time\n","\tcudaEvent_t start, stop;\n","\tcudaEventCreate(&start);\n","\tcudaEventCreate(&stop);\n","\n","\tuint n = 8*1024;         // matrix size\n","\tuint min_k = 10;         // min num of blocks\n","\tuint max_k = 100;         // max num of blocks\n","\n","\t// multiple tests for k = # diag blocks\n","\tfor (uint k = min_k; k <= max_k; k+=10) {\n","\t\tprintf(\"\\n*****   k = %d --- (avg block size = %f)\\n\",k,(float)n/k);\n","\t\ttestKernelsMQDB_unified(n, k, start, stop);\n","\t}\n","\n","  cudaEventDestroy(start);\n","\tcudaEventDestroy(stop);\n","\treturn 0;\n","}\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wLxZjCx8bT3s"},"source":["# Compilazione ed esecuzione\n","!nvcc -arch=sm_75  src/STREAMS/MQDB_stream_Unified.cu GPUcomputing/utils/MQDB/mqdb.cpp -o MQDBS\n","!./MQDBS"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6fYYbj397SdK"},"source":["%%cuda_group_save --name \"MQDB_stream_manual.cu\" --group \"STREAMS\"\n","\n","#include \"../../GPUcomputing/utils/MQDB/mqdb.h\"\n","#include \"../../GPUcomputing/utils/common.h\"\n","\n","#define BLOCK_SIZE 16     // block size\n","\n","/*\n"," * Kernel for block sub-matrix product of mqdb\n"," */\n","__global__ void mqdbBlockProd(mqdb A, mqdb B, mqdb C, uint sdim, uint d, uint n) {\n","\tint row = blockIdx.y * blockDim.y + threadIdx.y;\n","\tint col = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","\t// jump to the right block sub-matrix\n","\tuint  offset = (n+1)*sdim;\n","\n","\t// each thread computes an entry of the product matrix\n","\tif ((row < d) && (col < d)) {\n","\t\tfloat val = 0;\n","\t\tfor (int k = 0; k < d; k++)\n","\t\t\tval += A.elem[row * n + k + offset] * B.elem[k * n + col + offset];\n","\t\tC.elem[row * n + col + offset] = val;\n","\t}\n","}\n","\n","/*\n"," * Test on MQDB kernels using manual async memory\n"," */\n","void testKernelsMQDB_manual_mem(uint n, uint k, cudaEvent_t start, cudaEvent_t stop) {\n","\n","\t// matrices\n","\tmqdb *A, *B, *C;         // host\n","\tmqdb d_A, d_B, d_C;      // device\n","\n","  ulong nBytes = n * n * sizeof(float);\n","  int kBytes = k * sizeof(int);\n","\tprintf(\"Memory size required = %3.4f (MB)\\n\",(float)nBytes/(1024.0*1024.0));\n","\n","\n","  // host and device Memory\n","\tCHECK(cudaMallocHost(&A, sizeof(mqdb)));\n","  CHECK(cudaMallocHost(&A->blkSize, kBytes));\n","  CHECK(cudaMallocHost(&A->elem, nBytes));\n","  CHECK(cudaMalloc(&d_A.blkSize, kBytes));\n","  CHECK(cudaMalloc(&d_A.elem, nBytes));\n","\n","  CHECK(cudaMallocHost(&B, sizeof(mqdb)));\n","  CHECK(cudaMallocHost(&B->blkSize, kBytes));\n","  CHECK(cudaMallocHost(&B->elem, nBytes));\n","\tCHECK(cudaMalloc(&d_B.blkSize, kBytes));\n","  CHECK(cudaMalloc(&d_B.elem, nBytes));\n","\n","  CHECK(cudaMallocHost(&C, sizeof(mqdb)));\n","  CHECK(cudaMallocHost(&C->blkSize, kBytes));\n","  CHECK(cudaMallocHost(&C->elem, nBytes));\n","\tCHECK(cudaMalloc(&d_C.blkSize, kBytes));\n","  CHECK(cudaMalloc(&d_C.elem, nBytes));\n","\n","  // random fill mat entries\n","  int seed = 1;\n","\tgenRandDims(A, n, k, seed);\n","\tgenRandDims(B, n, k, seed);\n","\tgenRandDims(C, n, k, seed);\n","\tfillBlocks(A, n, k, 'C', 1);\n","\tfillBlocks(B, n, k, 'C', 2);\n","\tfillBlocks(C, n, k, 'C', 0);\n","\n","\t// copy blk sizes on device memory\n","\t//CHECK(cudaMemcpy(d_A->blkSize, A->blkSize, kBytes, cudaMemcpyHostToDevice));\n","\t//CHECK(cudaMemcpy(d_B->blkSize, B->blkSize, kBytes, cudaMemcpyHostToDevice));\n","\t//CHECK(cudaMemcpy(d_C->blkSize, C->blkSize, kBytes, cudaMemcpyHostToDevice));\n","\n","  /***********************************************************/\n","\t/*       GPU MQDB product using streams & async copy       */\n","\t/***********************************************************/\n","\tprintf(\"GPU MQDB product using streams...\\n\");\n","\n","\tdim3 block(BLOCK_SIZE, BLOCK_SIZE);\n","  int nstreams = A->nBlocks;\n","\tcudaStream_t streams[nstreams];\n","\tuint dsum = 0;  // bound dx\n","\tCHECK(cudaEventRecord(start));\n","\tfor (int i = 0; i < nstreams; i++) {\n","\t\tCHECK(cudaStreamCreate(&streams[i]));\n","\t\tuint d = A->blkSize[i];\n","    int offset = dsum*n;\n","    int streamBytes = d*n;\n","\t\tCHECK(cudaMemcpyAsync(&d_A.elem[offset], &A->elem[offset], streamBytes, cudaMemcpyHostToDevice, streams[i]));\n","    CHECK(cudaMemcpyAsync(&d_B.elem[offset], &B->elem[offset], streamBytes, cudaMemcpyHostToDevice, streams[i]));\n","\t\tdim3 grid((d + block.x - 1) / block.x, (d + block.y - 1) / block.y);\n","\t\tmqdbBlockProd<<<grid, block, 0, streams[i]>>>(d_A, d_B, d_C, dsum, d, n);\n","    CHECK(cudaMemcpyAsync(&C->elem[offset], &d_C.elem[offset], streamBytes, cudaMemcpyDeviceToHost, streams[i]));\n","\t\tdsum += d;\n","\t}\n","\tCHECK(cudaEventRecord(stop));\n","\tCHECK(cudaEventSynchronize(stop));\n","  float milliseconds;\n","\tCHECK(cudaEventElapsedTime(&milliseconds, start, stop));\n","\tfloat GPUtime3 = milliseconds / 1000.0;\n","\tprintf(\"   elapsed time                  : %.5f (sec)\\n\", GPUtime3);\n","\n","\t// clean up streams and events\n","\tfor (int i = 0; i < nstreams; i++)\n","\t\tcudaStreamDestroy(streams[i]);\n","\n","}\n","\n","/******************************************************************************/\n","/*                                  MAIN\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t*/\n","/******************************************************************************/\n","\n","int main(int argc, char *argv[]) {\n","\n","  // set up device\n","\tint dev = 0;\n","\tcudaDeviceProp deviceProp;\n","\tCHECK(cudaGetDeviceProperties(&deviceProp, dev));\n","\tprintf(\"%s starting mqdb product at \", argv[0]);\n","\tprintf(\"device %d: %s\\n\", dev, deviceProp.name);\n","\tCHECK(cudaSetDevice(dev));\n","\n","\t// events to measure time\n","\tcudaEvent_t start, stop;\n","\tcudaEventCreate(&start);\n","\tcudaEventCreate(&stop);\n","\n","\tuint n = 16*1024;         // matrix size\n","\tuint min_k = 20;       // max num of blocks\n","\tuint max_k = 30;       // max num of blocks\n","\n","\t// multiple tests for k = # diag blocks\n","\tfor (uint k = min_k; k <= max_k; k+=5) {\n","\t\tprintf(\"\\n*****   k = %d --- (avg block size = %f)\\n\",k,(float)n/k);\n","\t\ttestKernelsMQDB_manual_mem(n, k, start, stop);\n","\t}\n","\n","  cudaEventDestroy(start);\n","\tcudaEventDestroy(stop);\n","\treturn 0;\n","}\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HmMmj5KDnteq"},"source":["# Compilazione ed esecuzione\n","!nvcc -arch=sm_75  src/STREAMS/MQDB_stream_manual.cu GPUcomputing/utils/MQDB/mqdb.cpp -o MQDBS\n","!./MQDBS"],"execution_count":null,"outputs":[]}]}